{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f2169b",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"https://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/llibre-estil/logo-UOC-2linies.png\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.859 · Visualización de datos</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2023-1 · Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987878d3",
   "metadata": {},
   "source": [
    "<br><p style='font-size: 50px;'>Visualización de datos - PEC3</p><br><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Nombre y apellidos:</strong> Juan Gabriel Marinero Tarazona\n",
    "</div>\n",
    "<strong></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ba21b",
   "metadata": {},
   "source": [
    "<div id=\"table-of-content\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d821f",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4498d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, basename, isfile, join\n",
    "from importlib import import_module\n",
    "import glob\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "modules = glob.glob(join(current_dir+ '/ML_fcns', \"*.py\"))\n",
    "module_names = [basename(f)[:-3] for f in modules if isfile(f) and not f.endswith('__init__.py')]\n",
    "for module_name in module_names:\n",
    "    module = import_module(f\"ML_fcns.{module_name}\")\n",
    "\n",
    "    # Add the imported module to __all__\n",
    "    globals()[module_name] = getattr(module, module_name)\n",
    "    __all__ = module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31199d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_htmlFcn2 = lambda *args, **kwargs: display_htmlFcn(*args, _hide_index=True,  \n",
    "                                                           _hide_columns=True, _space=5, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fae627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_fcns.display_htmlFcn_applyMap_HideVals import display_htmlFcn_applyMap_HideVals as hideVals\n",
    "hideZeros = lambda val: hideVals(val,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fa449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softZeros(val):\n",
    "    return 'color:#D3D3D3;background-color:white' if val==0 else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_maxFcn(s, props='color:black;background-color:gold'):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03699c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotNN02(weights_history, biases_history, figsize=(12,5), radialBool=False, kwargs=dict()):\n",
    "    \n",
    "    if isinstance(weights_history, dict):\n",
    "        i = list(weights_history.keys())[-1] # key_last_iter\n",
    "        w_i, w_j = [weights_history.get(i).get(k) for k in ['w_1','w_2']]\n",
    "        b_i, b_j = [biases_history.get(i).get(k)  for k in ['b_1','b_2']]\n",
    "    else:\n",
    "        w_i, w_j = [weights_history[-1].get(k) for k in ['w_1','w_2']]\n",
    "        b_i, b_j = [biases_history[-1].get(k)  for k in ['b_1','b_2']]\n",
    "\n",
    "    # making of:\n",
    "    # plotNN01([2,4], weights=np.array(w_i)[np.newaxis,:], biases=b_i, figsize=(10,3), radialBool=False);\n",
    "    # plotNN01([4,1], weights=np.array(w_j)[np.newaxis,:], biases=b_j, figsize=(5,3), radialBool=False);\n",
    "    \n",
    "    w_i = np.array(w_i)[np.newaxis,:][0]\n",
    "    b_i = np.array(b_i)[0]\n",
    "    w = [w_i]\n",
    "    b = [b_i]\n",
    "    if w_j is not None:\n",
    "        w_j = np.array(w_j)[np.newaxis,:][0]\n",
    "        b_j = np.array(b_j)[0]\n",
    "        w.append(w_j)\n",
    "        b.append(b_j)\n",
    "\n",
    "    cells = [len(w[0])]\n",
    "    cells.extend([len(k) for k in b])\n",
    "\n",
    "\n",
    "    return plotNN01(cells, weights=w, biases=b, figsize=figsize, radialBool=radialBool, \n",
    "                    arrow_opacity_fcn=lambda x: 0.3, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711300d",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d8edb",
   "metadata": {},
   "source": [
    "Main <a href=\"https://dustinstansbury.github.io/theclevermachine/a-gentle-introduction-to-neural-networks\">reference</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc0f92",
   "metadata": {},
   "source": [
    "## Single-layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bd0f3",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"width:300px;padding-left:45px;\" src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/perceptron2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7b3ae",
   "metadata": {},
   "source": [
    "### activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466102d9",
   "metadata": {},
   "source": [
    "$$g_{linear}(z)=z$$\n",
    "\n",
    "which outputs continuous values $a_{linear} \\in (-\\infty,\\infty)$\n",
    ", then the network implements a linear model akin to used in standard linear regression.\n",
    "\n",
    "\n",
    "\n",
    "$$g_{logistic}(z)=\\dfrac{1}{1+e^{-z}}$$\n",
    "\n",
    "which outputs values $a_{logistic} \\in (0,1)$\n",
    "\n",
    "Binary classification can also be implemented using the **hyperbolic tangent** function\n",
    "\n",
    "$$tanh(z)$$\n",
    "\n",
    "which outputs values $a_{tanh} \\in (−1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_htmlFcn_plots_activation_fcn_statics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c819a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_htmlFcn_plots_activation_fcn_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32517703",
   "metadata": {},
   "source": [
    "- The **logistic** and **tanh** activation functions (blue and green) have the quintessential sigmoidal “s” shape that **saturates for inputs of large magnitude**. This behavior makes them useful for **categorization**. \n",
    "- The **identity  linear** activation (red), however forms a **linear mapping** between the input to the activation function, which makes it useful for **predicting continuous** values. Example [here](#Neural-Networks-for-Regression).\n",
    "\n",
    "A key property of these activation functions is that they are all smooth and differentiable. We’ll see later in this post why <u>differentiability is important for training neural networks</u>. The derivatives for each of these common activation functions are given by (for mathematical details on calculating these derivatives, see this [post](https://dustinstansbury.github.io/theclevermachine/derivation-common-neural-network-activation-functions)):\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "g′_{linear}(z)&=1\\\\\n",
    "g′_{logistic}(z)&=g_{logistic}(z)(1−g_{logistic}(z))\\\\\n",
    "g′_{tanh}(z)&=1−g^2_{tanh}(z)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "These **derivatives** are either a constant (i.e. 1), or are can be **defined in terms of the original function**. This makes them extremely convenient for **efficiently** training neural networks, as we can implement the gradient using **simple manipulations of the feed-forward states of the network**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b05c5b",
   "metadata": {},
   "source": [
    "## Multi-layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66ca91",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img align=\"center\" style=\"width:500px;padding-left:45px;\" src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/multi-layer-perceptron.png\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bbe97",
   "metadata": {},
   "source": [
    "Multi-layer neural networks form compositional functions that map the inputs nonlinearly to outputs. If we associate:\n",
    "- index $i$ with the input layer, \n",
    "- index $j$ with the hidden layer, and \n",
    "- index $k$ with the output layer, \n",
    "\n",
    "$$\n",
    "a_{out}=a_k\n",
    "=g_k (z_k)\n",
    "=g_k \\left(b_k+\\sum_j a_j      \\cdot w_{jk}\\right)\n",
    "=g_k \\left(b_k+\\sum_j g_j(z_j) \\cdot w_{jk}\\right)\n",
    "=g_k \\left(b_k+\\sum_j g_j\\left(b_j+\\sum_i a_i w_{ij}\\right)w_{jk}\\right)\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "- $z_l$ is the pre-activation values for the units in layer $l$    \n",
    "- $g_l()$ is the activation function for units in layer $l$ (assuming the same function for all units)\n",
    "- $a_l=g_l(z_l)$ is the output activation for units in layer $l$\n",
    "- $w_{l−1,l}$ are the parameters that weight the output messages of units feeding into layer $l$ to the activation function of units for that layer.\n",
    "- $b_l$ term is the bias/DC offset for units in layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf3b32",
   "metadata": {},
   "source": [
    "### Training neural networks & gradient descent\n",
    "\n",
    "Training neural networks involves determining the model parameters $\\theta=\\{w,b\\}$\n",
    "that minimize the errors the network makes. This first requires that we have a way of quantifying error. A standard way of quantifying error is to take the squared difference between the network output and the target value:\n",
    "\n",
    "$$\n",
    "E=\\dfrac{(output−target)^2}{2}\n",
    "$$\n",
    "\n",
    "With an **error function** in hand, we then aim to **find the setting of parameters that minimizes this error function**, when aggregated across all the training data. This concept can be interpreted spatially by imagining a “parameter space” whose dimensions are the values of each of the model parameters, and for which the error function will form a surface of varying height depending on its value for each parameter. Model <u>training is thus equivalent to finding point in parameter space (like a meshgrid) that makes the height of the error surface small</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382ac52",
   "metadata": {},
   "source": [
    "## Analysis of simple neural networks\n",
    "\n",
    "### Single-layer neural network\n",
    "\n",
    "<img align=\"right\" style=\"width:600px;padding-left:45px;\" src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/single-layer-ann-error-surface.png\">\n",
    "\n",
    "To get a better intuition behind the concept of minimizing the error surface, let’s define a super-simple neural network, one that has a single input and a single output. For further simplicity, we’ll assume the network has no bias term and thus has a single parameter, $w_1$. We will also assume that the output layer uses the logistic sigmoid activation function. Accordingly, the network will map some input value $a_0$ onto a predicted output $a_{out}$ via the following function.\n",
    "\n",
    "$$\n",
    "a_{out}=g_{logistic}(a_0 w_1)\n",
    "$$\n",
    "\n",
    "Now let’s say we want this simple network to learn the identity function: given an input of 1 it should return a target value of 1. Given this target value we can now calculate the value of the error function for each setting of $w_1$. Varying the value of $w_1$ from -10 to 10 results in the error surface displayed in the right of Figure 4. We see that:\n",
    "- the error is small for large positive values of $w_1$, \n",
    "- while the error is large for strongly negative values of $w_1$. \n",
    "\n",
    "This not surprising, given that the output activation function is the logistic sigmoid, which will map large values onto an output of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Define a few common activation functions\n",
    "g_linear = lambda z: z\n",
    "g_sigmoid = lambda z: 1./(1. + np.exp(-z))\n",
    "g_tanh = lambda z: np.tanh(z)\n",
    "   \n",
    "# ...and their analytic derivatives    \n",
    "g_prime_linear = lambda z: np.ones(len(z))\n",
    "g_prime_sigmoid = lambda z: 1./(1 + np.exp(-z)) * (1 - 1./(1 + np.exp(-z)))\n",
    "g_prime_tanh = lambda z: 1 - np.tanh(z) ** 2\n",
    "\n",
    "activation_functions = OrderedDict(\n",
    "    [\n",
    "        (\"linear\", (g_linear, g_prime_linear, 'red')),\n",
    "        (\"sigmoid\", (g_sigmoid, g_prime_sigmoid, 'blue')),\n",
    "        (\"tanh\", (g_tanh, g_prime_tanh, 'green')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de98f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_range_arr = [5,10,30]  \n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "for i,k in enumerate(parameter_range_arr):\n",
    "    ax = fig.add_subplot(1, len(parameter_range_arr), i+1, projection='3d')\n",
    "    plotSurfaceError_01(ax, k)\n",
    "plt.suptitle('Single-layer Network\\nError Surface', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSurfaceError_02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSurfaceError_03()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2be4f3",
   "metadata": {},
   "source": [
    "### Multi-layer neural network\n",
    "\n",
    "<img align=\"right\" style=\"width:600px;padding-left:45px;\" src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/multi-layer-ann-error-surface.png\">\n",
    "Things become more interesting when we move from a single-layered network to a multi-layered network. Let’s repeat the above exercise, but include a single hidden node between the input and the output. Again, we will assume no biases, and logistic sigmoid activations for both the hidden and output nodes. Thus the network will have two parameters: $(w_1,w_2)$. Accordingly the 2-layered network will predict an output with the following function:\n",
    "\n",
    "$$\n",
    "a_{out}=g_{logistic}\\left(g_{logistic}(a_0 w_1)w_2\\right)\n",
    "$$\n",
    "\n",
    "Now, if we vary both $w_1$ and $w_2$, we obtain the error surface in right of Figure 5.\n",
    "\n",
    "We see that the error function is minimized when both $w_1$ and $w_2$ are large and positive. We also see that the error surface is more complex than for the single-layered model, exhibiting a number of wide plateau regions. It turns out that the error surface gets more and more complicated as you increase the number of layers in the network and the number of units in each hidden layer. Thus, it is important to consider these phenomena when constructing neural network models.\n",
    "\n",
    "The examples in Figures 4-5 gives us a qualitative idea of how to train the parameters of an NN, but we would like a more automatic way of doing so. Generally this problem is solved using **gradient descent**. The gradient descent algorithm (lee lo de Montecarlo en asig. eléctrica SEE):\n",
    "- first calculates the derivative / gradient of the error function with respect to each of the model parameters. This gradient information will give us the direction in parameter space that decreases the height of the error surface.\n",
    "- We then take a step in that direction \n",
    "- and repeat, iteratively calculating the gradient and taking steps in parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e195d92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def two_layer_network_predict(w1, w2, target_value=1):\n",
    "    return g_sigmoid(w2 * g_sigmoid(w1 * target_value))\n",
    "\n",
    "parameter_range_arr = [5,10,30]  \n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "for i,k in enumerate(parameter_range_arr):\n",
    "    ax = fig.add_subplot(1, len(parameter_range_arr), i+1, projection='3d')\n",
    "    plotSurfaceError_01(ax, k, layer_network_predict=two_layer_network_predict, set_xlabel_bool=True,\n",
    "                    view_angle=[25, 45])\n",
    "plt.suptitle('Multi-layer Network\\nError Surface', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52d717",
   "metadata": {},
   "source": [
    "### The Backpropagation Algorithm\n",
    "\n",
    "<img align=\"right\" style=\"width:500px;padding-left:45px;\" src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/backpropagation-steps.png\">\n",
    "<figcaption>Figure 3: Diagram of a multi-layer ANN. Each node in the network can be considered a single-layered ANN (for simplicity, biases are not visualized in graphical model)</figcaption>\n",
    "\n",
    "It turns out that the gradient information for the NN error surface can be calculated efficiently using a message passing algorithm known as the **backpropagation** algorithm:\n",
    "- input signals are forward-propagated through the network toward the outputs, and \n",
    "- network errors are then calculated with respect to target variables and “backpropagated” backwards towards the inputs.\n",
    "\n",
    "The <u>forward and backward signals</u> are then used to <u>determine the direction</u> in the parameter space to move <u>that lowers the network error</u>.\n",
    "\n",
    "Figure 6 demonstrates the **4 key steps of the backpropagation** algorithm. The main concept: for a given observation we want to determine the degree of “responsibility” that each network parameter has for mis-predicting a target value associated with the observation. We then change that parameter according to this responsibility so that it reduces the network error.\n",
    "\n",
    "To train an ANN, the 4 steps in Figure 6 are repeated iteratively by observing many input-target pairs and updating the parameters until:\n",
    "- either the network error reaches a tolerably low value, \n",
    "- the parameters cease to update (convergence), or \n",
    "- a set number of iterations over the training data has been achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2293f31b",
   "metadata": {},
   "source": [
    "## Neural Networks for Classification\n",
    "\n",
    "<img align=\"right\" style=\"width:200px;padding-left:25px;\" src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/truth-table.png\">\n",
    "\n",
    "Here we go over an example of training a single-layered neural network to perform a classification problem. The network is trained to learn a set of logical operators including the AND, OR, or XOR. To train the network we first generate training data. The inputs consist of 2-dimensional coordinates that span the input values $(x_1,x_2)$ values for a 2-bit truth table:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c858f3",
   "metadata": {},
   "source": [
    "### `generate_classification_data` and `generate_regression_data`\n",
    "\n",
    "We then perturb these observations by adding Normally-distributed noise. To generate target variables, we categorize each observations by applying one of logic operators (described in Figure 7) to the original (no-noisy) coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(problem_type, n_obs_per_class=30):\n",
    "    \"\"\"Generates training data for all demos\n",
    "    \"\"\"\n",
    "    np.random.seed(123)\n",
    "\n",
    "    truth_table = np.array( [ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "    ring_table = np.vstack( [ truth_table, np.array( [ [.5, .5], [1., .5], [0., .5], [.5, 0.], [.5, 1.] ]) ])\n",
    "    ring_classes = [1., 1., 1., 1., 0., 1., 1., 1., 1.];\n",
    "\n",
    "    problem_classes = {\n",
    "        'AND': np.logical_and(truth_table[:,0], truth_table[:, 1]) * 1.,\n",
    "        'OR':  np.logical_or( truth_table[:,0], truth_table[:, 1]) * 1.,\n",
    "        'XOR': np.logical_xor(truth_table[:,0], truth_table[:, 1]) * 1.,\n",
    "        'RING': ring_classes\n",
    "    }\n",
    "\n",
    "    if problem_type in ('AND', 'OR', 'XOR'):\n",
    "        observations = np.tile(truth_table, (n_obs_per_class, 1)) + .15 * np.random.randn(n_obs_per_class * 4, 2)\n",
    "        obs_classes = np.tile(problem_classes[problem_type], n_obs_per_class)\n",
    "    else:\n",
    "        observations = np.tile(ring_table, (n_obs_per_class, 1)) + .15 * np.random.randn(n_obs_per_class * 9, 2)\n",
    "        obs_classes = np.tile(problem_classes[problem_type], n_obs_per_class)\n",
    "\n",
    "    # Permute data\n",
    "    permutation_idx = np.random.permutation(np.arange(len(obs_classes)))\n",
    "    obs_classes = obs_classes[permutation_idx]\n",
    "    observations = observations[permutation_idx]\n",
    "    obs_x = [obs[0] for obs in observations]\n",
    "    obs_y = [obs[1] for obs in observations]\n",
    "    return obs_x, obs_y, obs_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820b913",
   "metadata": {},
   "source": [
    "Explanation of call to `generate_classification_data()`:\n",
    "\n",
    "- observe final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d65665",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type='AND'\n",
    "n_obs_per_class=3\n",
    "obs_x, obs_y, obs_classes = generate_classification_data(problem_type=problem_type, \n",
    "                                                         n_obs_per_class=n_obs_per_class)\n",
    "df = pd.DataFrame([problem_type, n_obs_per_class, int(len(obs_x)/n_obs_per_class)], \n",
    "                  index=[\"problem_type\", \"n_obs_per_class\", \"len(obs_x)/n_obs_per_class\"])\n",
    "display_htmlFcn2(\"df\", \"obs_x\", \"obs_y\", \n",
    "                 \"obs_classes\", # its calculus explained in cells below\n",
    "                 \"np.rint(obs_x)\",\"np.rint(obs_y)\",\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[None,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7904c",
   "metadata": {},
   "source": [
    "- understand `problem_classes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_table = np.array( [ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "ring_table = np.vstack( [ truth_table, np.array( [ [.5, .5], [1., .5], [0., .5], [.5, 0.], [.5, 1.] ]) ])\n",
    "ring_classes = [1., 1., 1., 1., 0., 1., 1., 1., 1.]\n",
    "\n",
    "problem_classes = {\n",
    "    'AND': np.logical_and(truth_table[:,0], truth_table[:, 1]) * 1.,\n",
    "    'OR':  np.logical_or( truth_table[:,0], truth_table[:, 1]) * 1.,\n",
    "    'XOR': np.logical_xor(truth_table[:,0], truth_table[:, 1]) * 1.,\n",
    "    'RING': ring_classes\n",
    "}\n",
    "\n",
    "# RING plot\n",
    "plt.ioff() # prevent plots from being displayed in the output of Jupyter Notebook\n",
    "fig, ax = plt.subplots()  \n",
    "ax.scatter(*ring_table.T, c=ring_classes)\n",
    "ax.tick_params(labelsize=7); ax.set_xticks(ax.get_xticks()[::2]); ax.set_yticks(ax.get_yticks()[::2])\n",
    "plt.ion() # revert plt.ioff()\n",
    "html_plot_fig = display_htmlFcn_plots(fig, width=190.0, height=200.0, caption=\"ax.scatter(*ring_table.T, c=ring_classes)\")\n",
    "\n",
    "display_htmlFcn2(\"truth_table\", \"problem_classes['AND']\", \"problem_classes['OR']\",\"problem_classes['XOR']\",\n",
    "                 \"ring_table\",\"problem_classes['RING']\", html_plot_fig,\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[0,0,0,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0bf6b",
   "metadata": {},
   "source": [
    "- how `obs_classes` (pre-permutated) is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs_per_class=3\n",
    "problem_type = 'XOR'\n",
    "problem_type_arr = problem_classes[problem_type]\n",
    "observations = np.tile(truth_table, (n_obs_per_class, 1)) \\\n",
    "               + .15 * np.random.randn(n_obs_per_class * len(problem_type_arr), 2) # repeat input + add noise\n",
    "obs_classes = np.tile(problem_type_arr, n_obs_per_class) # repeat target (no noisy)\n",
    "display_htmlFcn2(\"problem_classes['XOR']\", \"observations\", \"obs_classes\", \n",
    "                 \"np.rint(observations)\", # rint to notice how each |noise| of \"observations\" is <<<1\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04e9d2",
   "metadata": {},
   "source": [
    "- finally permutate vars to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute data\n",
    "permutation_idx = np.random.permutation(np.arange(len(obs_classes))) # np.random.permutation([1, 4, 9, 12, 15]) = array([15,  1,  9,  4, 12]) # random\n",
    "obs_classes = obs_classes[permutation_idx]\n",
    "observations = observations[permutation_idx]\n",
    "obs_x = [obs[0] for obs in observations]\n",
    "obs_y = [obs[1] for obs in observations]\n",
    "display_htmlFcn2(\"permutation_idx\",\"obs_x\", \"obs_y\", \"obs_classes\", \"np.rint(observations)\",\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d778cee",
   "metadata": {},
   "source": [
    "Explain `generate_regression_data()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regression_data(problem_type='SIN', n_obs=100):\n",
    "    np.random.seed(123)\n",
    "    xx = np.linspace(-5, 5, n_obs);\n",
    "    if problem_type == 'SIN':\n",
    "        f = lambda x: 2.5 + np.sin(x)\n",
    "    elif problem_type == 'ABS':\n",
    "        f = lambda x: abs(x)\n",
    "\n",
    "    yy = f(xx) + np.random.randn(*xx.shape)*.5\n",
    "    perm_idx = np.random.permutation(np.arange(n_obs))\n",
    "    return xx[perm_idx, None], yy[perm_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_type='SIN'\n",
    "n_obs=10\n",
    "np.random.seed(123)\n",
    "xx = np.linspace(-5, 5, n_obs)\n",
    "\n",
    "f = lambda x: 2.5 + np.sin(x)\n",
    "\n",
    "yy = f(xx) + np.random.randn(*xx.shape)*.5 # sin(xx) + noise\n",
    "perm_idx = np.random.permutation(np.arange(n_obs))\n",
    "\n",
    "x, y = generate_regression_data(problem_type, n_obs=100) # call to generate_regression_data !!\n",
    "p = np.poly1d(np.polyfit(x.flatten(), y.flatten(), 3))\n",
    "t = np.linspace(min(x), max(x), 20)\n",
    "\n",
    "plt.ioff() # prevent plots from being displayed in the output of Jupyter Notebook\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(x, y, 'o', t, p(t), '-')\n",
    "plt.ion() # revert plt.ioff()\n",
    "html_plot_fig = display_htmlFcn_plots(fig, width=250.0, height=250.0, caption=\"ax.plot(x, y, 'o', t, p(t), '-')\")\n",
    "\n",
    "\n",
    "display_htmlFcn2(\"perm_idx\", \"xx[perm_idx, None]\",\"yy[perm_idx]\", html_plot_fig,\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2d153",
   "metadata": {},
   "source": [
    "### `initialize_network_parameters`\n",
    "\n",
    "With training data in hand, we then train the network with the noisy inputs and binary categories targets using the gradient descent / backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac670f5",
   "metadata": {},
   "source": [
    "1st get from `generate_classification_data` the `observations` (inputs of ANN) and `targets` (desired outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056024e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some args of run_ann_training_simulation\n",
    "problem_type='AND'\n",
    "n_observations=3\n",
    "\n",
    "# start of run_ann_training_simulation\n",
    "obs_x, obs_y, targets = generate_classification_data(problem_type, n_obs_per_class=n_observations)\n",
    "observations = np.vstack([obs_x, obs_y]).T\n",
    "\n",
    "print(f\"problem_type: {problem_type}\")\n",
    "display_htmlFcn2(\"obs_x\", \"obs_y\",\"targets\", \"np.rint(observations)\", \"observations\",\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[1,1,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a906a61",
   "metadata": {},
   "source": [
    "With the `observations` we can call `initialize_network_parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network_parameters(n_input_units, n_hidden_units=0, n_output_units=1):\n",
    "    \"\"\"Generate weights and bias parameters based on defined network architecture\"\"\"\n",
    "    w1_size = n_hidden_units if n_hidden_units > 0 else n_output_units\n",
    "    # Weights\n",
    "    weights = dict()\n",
    "    weight_gradients = dict()\n",
    "\n",
    "    weights['w_1'] = np.random.rand(n_input_units, w1_size) - .5\n",
    "    weight_gradients['w_1'] = np.zeros_like(weights['w_1'])\n",
    "\n",
    "    if n_hidden_units > 0:\n",
    "        weights['w_2'] = np.random.rand(n_hidden_units, n_output_units) - .5\n",
    "        weight_gradients['w_2'] = np.zeros_like(weights['w_2'])\n",
    "\n",
    "    # Biases\n",
    "    biases = dict()\n",
    "    bias_gradients = dict()\n",
    "    biases['b_1'] = np.random.rand(w1_size) - .5\n",
    "    bias_gradients['b_1'] = np.zeros_like(biases['b_1'])\n",
    "\n",
    "    if n_hidden_units > 0:\n",
    "        biases['b_2'] = np.random.rand(n_output_units) - .5\n",
    "        bias_gradients['b_2'] = np.zeros_like(biases['b_2'])\n",
    "\n",
    "    return weights, biases, weight_gradients, bias_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0be039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some args of run_ann_training_simulation\n",
    "n_hidden_units=0 # neurons in 1st hidden layer\n",
    "\n",
    "# block of run_ann_training_simulation\n",
    "# Initialize model parameters $\\theta$\n",
    "n_output_dims = 1\n",
    "n_obs, n_input_dims = observations.shape\n",
    "weights, biases, weight_gradients, bias_gradients = initialize_network_parameters(\n",
    "    n_input_dims, n_hidden_units, n_output_dims\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases, weight_gradients, bias_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb542a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig_NN_2_1 = plotNN01([2,1], weights=list(weights.values()), biases=list(biases.values()), figsize=(5,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = copy.deepcopy(weights)\n",
    "w['w_1'][0][0] *= -1\n",
    "display(w)\n",
    "plotNN01([2,1], weights=list(w.values()), biases=list(biases.values()), figsize=(5,3),\n",
    "         arrow_color_fcn=lambda x: \"green\" if x > 0 else \"red\",\n",
    "         showVals=False, # to hide vals\n",
    "         arrow_opacity_fcn=lambda x: .3,\n",
    "         arrow_width = 0.001*50, arrow_width_variable = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b63eb24",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "print(f\"n_hidden_units: {n_hidden_units} (0 means no hidden layer, >0 are neurons in 1st hidden layer)\")\n",
    "for w,b in zip(weights.keys(),biases.keys()):\n",
    "    display_htmlFcn2(f\"weights['{w}']\", f\"biases['{b}']\",f\"weight_gradients['{w}']\", f\"bias_gradients['{b}']\",\n",
    "                     _localVars=locals(), _applyOrApplyMap=softZeros, _formatVal=[\"{:.3f}\",\"{:.3f}\",\"{:.1e}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c761190",
   "metadata": {},
   "source": [
    "Explanation of\n",
    "\n",
    "```python\n",
    "weights, biases, weight_gradients, bias_gradients = initialize_network_parameters(\n",
    "    n_input_dims, n_hidden_units, n_output_dims\n",
    ")\n",
    "```\n",
    "\n",
    "Remember:\n",
    "\n",
    "```python\n",
    "def initialize_network_parameters(n_input_units, n_hidden_units=0, n_output_units=1):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a977514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_network_parameters\n",
    "n_input_units = n_input_dims\n",
    "print(f\"n_input_units:  {n_input_units}\")\n",
    "print(f\"n_hidden_units: {n_hidden_units} (0 means no hidden layer, >0 are neurons in 1st hidden layer)\")\n",
    "n_output_units = n_output_dims\n",
    "print(f\"n_output_units: {n_output_units}\")\n",
    "\n",
    "w1_size = n_hidden_units if n_hidden_units > 0 else n_output_units\n",
    "print(f\"w1_size: {w1_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "weights = dict()\n",
    "weight_gradients = dict()\n",
    "\n",
    "# n_cols dependent on w1_size (n_hidden_units if not 0, else n_output_units)\n",
    "weights['w_1'] = np.random.rand(n_input_units, w1_size) - .5 # random uniform distrib. over [-.5, .5) = [0, 1) - .5\n",
    "weight_gradients['w_1'] = np.zeros_like(weights['w_1']) # zeros\n",
    "\n",
    "if n_hidden_units > 0: # False in this example\n",
    "    weights['w_2'] = np.random.rand(n_hidden_units, n_output_units) - .5 # same\n",
    "    weight_gradients['w_2'] = np.zeros_like(weights['w_2']) # same\n",
    "    \n",
    "for w in weights.keys():\n",
    "    display_htmlFcn2(f\"weights['{w}']\", f\"weight_gradients['{w}']\",\n",
    "                     _localVars=locals(), _applyOrApplyMap=softZeros, _formatVal=[\"{:.3f}\",\"{:.1e}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases\n",
    "biases = dict()\n",
    "bias_gradients = dict()\n",
    "biases['b_1'] = np.random.rand(w1_size) - .5 # no dependent on n_input_units as weights['w_1'] was\n",
    "bias_gradients['b_1'] = np.zeros_like(biases['b_1'])\n",
    "\n",
    "if n_hidden_units > 0: # False in this example\n",
    "    biases['b_2'] = np.random.rand(n_output_units) - .5\n",
    "    bias_gradients['b_2'] = np.zeros_like(biases['b_2'])\n",
    "\n",
    "for w in biases.keys():\n",
    "    display_htmlFcn2(f\"biases['{w}']\", f\"bias_gradients['{w}']\",\n",
    "                     _localVars=locals(), _applyOrApplyMap=softZeros, _formatVal=[\"{:.3f}\",\"{:.1e}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62ae98",
   "metadata": {},
   "source": [
    "### `run_ann_training_simulation`\n",
    "\n",
    "\"ann\" from Artificial Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecb0ba",
   "metadata": {},
   "source": [
    "Next code block of `run_ann_training_simulation` (activation functions defined [here](#activation-function)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize problem-specific activation functions and their derivatives\n",
    "g_activation = {}\n",
    "g_activation_prime = {}\n",
    "if problem_type in ('SIN', 'ABS'):  # regression using linear output (and optional tanh hidden) activations\n",
    "    g_activation['g_out'], g_activation_prime['g_out'], _ = activation_functions['linear']\n",
    "    if 'w_2' in weights:\n",
    "        g_activation['g_1'], g_activation_prime['g_1'], _ = activation_functions['tanh']\n",
    "else:  # classification using all sigmoid activations\n",
    "    g_activation['g_out'], g_activation_prime['g_out'], _ = activation_functions['sigmoid']\n",
    "    if 'w_2' in weights:\n",
    "        g_activation['g_1'], g_activation_prime['g_1'], _ = activation_functions['sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect \n",
    "\n",
    "def getLambdaFcn(fcn = \"g_activation['g_out']\"):\n",
    "    print(f\"{fcn:27}\", end=\" -->  \")\n",
    "    print(inspect.getsource(eval(fcn)), end=\"\")\n",
    "    \n",
    "getLambdaFcn()\n",
    "getLambdaFcn(fcn = \"g_activation_prime['g_out']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd176063",
   "metadata": {},
   "source": [
    "Next code block is another setup, not used till end of function block `# Keep learning history for visualization` inside the `# Run the training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6053b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars defined later when declared visualize_classification_learning()\n",
    "PREDICTION_SURFACE_RESOLUTION = 20\n",
    "PREDICTION_COLORMAP = 'spring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for learning history / visualization\n",
    "loss_history = []\n",
    "prediction_history = {}\n",
    "weights_history = {}\n",
    "biases_history = {}\n",
    "if problem_type in ('SIN', 'ABS'):\n",
    "    prediction_x = np.linspace(-5, 5, PREDICTION_SURFACE_RESOLUTION)\n",
    "else:\n",
    "    prediction_surface_range = np.linspace(-.5, 1.5, PREDICTION_SURFACE_RESOLUTION)\n",
    "    prediction_surface_x, prediction_surface_y = np.meshgrid(prediction_surface_range, prediction_surface_range)\n",
    "    prediction_surface_xy = [(x, y) for x, y in zip(prediction_surface_x.ravel(), prediction_surface_y.ravel())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71989c2",
   "metadata": {},
   "source": [
    "Summary of results before `# Run the training`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n_observations: {n_observations}\")\n",
    "\n",
    "print(f\"problem_type: {problem_type}\")\n",
    "\n",
    "plt.ioff() # prevent plots from being displayed in the output of Jupyter Notebook\n",
    "fig, ax = plt.subplots()  \n",
    "ax.scatter(*observations.T, c=targets)\n",
    "ax.tick_params(labelsize=7); aux=np.linspace(-.5,1.5,5); ax.set_xticks(aux); ax.set_yticks(aux)\n",
    "ax.grid(False)\n",
    "plt.ion() # revert plt.ioff()\n",
    "html_plot_fig = display_htmlFcn_plots(fig, width=190.0, height=200.0, \n",
    "                                      caption=\"ax.scatter(*observations.T, c=targets)\")\n",
    "\n",
    "html_plot_fig2 = display_htmlFcn_plots(fig_NN_2_1,  width=150.0, height=200.0, \n",
    "                                       caption=\"Initial random weights & biases\")\n",
    "# remove grid-axis\n",
    "id_fig = \"fig2\"\n",
    "html_plot_fig2 = f'<style>#{id_fig} .mpld3-xaxis, #{id_fig} .mpld3-yaxis, #{id_fig}' \\\n",
    "+ ' .mpld3-grid { display: none; }</style>' + html_plot_fig2\n",
    "html_plot_fig2 = f'<div style=\"display:inline;\" id=\"{id_fig}\">{html_plot_fig2}</div>'\n",
    "\n",
    "display_htmlFcn2(\"obs_x\", \"obs_y\",\"targets\", \"np.rint(observations)\", \"observations\", \n",
    "                 html_plot_fig, html_plot_fig2,\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[1,1,0,0,1])\n",
    "\n",
    "print(f\"n_hidden_units: {n_hidden_units}\")\n",
    "for w,b in zip(weights.keys(),biases.keys()):\n",
    "    display_htmlFcn2(f\"weights['{w}']\", f\"biases['{b}']\",f\"weight_gradients['{w}']\", f\"bias_gradients['{b}']\",\n",
    "                     _localVars=locals(), _applyOrApplyMap=softZeros, _formatVal=[\"{:.3f}\",\"{:.3f}\",\"{:.1e}\"])\n",
    "\n",
    "getLambdaFcn()\n",
    "getLambdaFcn(fcn = \"g_activation_prime['g_out']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b334b62",
   "metadata": {},
   "source": [
    "#### Step I code\n",
    "\n",
    "**Step I: forward-propagate input signal**\n",
    "\n",
    "To forward-propagate the observed input forward through the network layers in order to provide a prediction for the current target $t$.\n",
    "\n",
    "Note that in the Figure 6-I:\n",
    "- $a_k$ could be considered network output (for a network with one hidden layer) or the output of a hidden layer that projects the remainder of the network (in the case of a network with more than one hidden layer). For this discussion, however, <u>we assume that the index $k$ is associated with the output layer</u> of the network, and thus each of the network outputs is designated by $a_k$\n",
    "- when implementing this forward-propagation step, we should keep track of the feed-forward pre-activations $z_l$ and activations $a_l$ for all layers $l$, as these can be used to efficiently calculate backpropagated errors and error function gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7fcd6",
   "metadata": {},
   "source": [
    "Sidenote: **Matrix Multiplication vs Element-wise Multiplication**.  \n",
    "\n",
    "From [documentation](https://docs.python.org/3/reference/simple_stmts.html#augmented-assignment-statements):\n",
    "> The `@` (at) operator is intended to be used for **matrix multiplication**.  [But not all (or maybe still none) builtin Python types implement this operator.]\n",
    "\n",
    "From [stackoverflow](https://stackoverflow.com/a/30629255/9391770):\n",
    "> `@` and `@=` are meant to clarify the confusion which existed so far with the operator `*` which was used either for element-wise multiplication or matrix multiplication depending on the convention employed in that particular library/code. As a result, in the future, the operator `*` is meant to be used for element-wise multiplication only. \n",
    "\n",
    "```\n",
    "A = [[1, 2],    B = [[11, 12],\n",
    "     [3, 4]]         [13, 14]]\n",
    "\n",
    "A * B = [[1 * 11,   2 * 12], \n",
    "         [3 * 13,   4 * 14]]\n",
    "\n",
    "A @ B  =  [[1 * 11 + 2 * 13,   1 * 12 + 2 * 14],\n",
    "           [3 * 11 + 4 * 13,   3 * 12 + 4 * 14]]\n",
    "```\n",
    "           \n",
    "**`@`** operator makes the code involving matrix multiplications much easier to read:\n",
    "\n",
    "```python\n",
    "# Current implementation of matrix multiplications using dot function\n",
    "S = np.dot((np.dot(H, beta) - r).T,\n",
    "            np.dot(inv(np.dot(np.dot(H, V), H.T)), np.dot(H, beta) - r))\n",
    "\n",
    "# Current implementation of matrix multiplications using dot method\n",
    "S = (H.dot(beta) - r).T.dot(inv(H.dot(V).dot(H.T))).dot(H.dot(beta) - r)\n",
    "\n",
    "# Using the @ operator instead\n",
    "S = (H @ beta - r).T @ inv(H @ V @ H.T) @ (H @ beta - r)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_I_forwardprop(network_input, weights, biases, g_activation):\n",
    "    if 'w_2' in weights:  # multi-layer network\n",
    "        z_hidden = network_input @ weights['w_1'] + biases['b_1']\n",
    "        a_hidden = g_activation['g_1'](z_hidden)\n",
    "        z_output = a_hidden @ weights['w_2'] + biases['b_2']\n",
    "    else:  # single-layer network\n",
    "        z_hidden = np.array([])\n",
    "        a_hidden = np.array([])\n",
    "        z_output = network_input @ weights['w_1'] + biases['b_1']\n",
    "    \n",
    "    a_output = g_activation['g_out'](z_output) # Network prediction\n",
    "    return a_output, z_output, a_hidden, z_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df7a92",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"width:400px;padding-left:45px;\" src=\"https://dustinstansbury.github.io/theclevermachine/assets/images/a-gentle-introduction-to-neural-networks/multi-layer-perceptron.png\">\n",
    "\n",
    "$$\n",
    "\\begin{array}{llll}\n",
    "a_{out}=a_k\n",
    "&=g_k \\bigg(b_k+\\sum_j \\bigg(g_j\\big(b_j+\\sum_i a_i w_{ij}\\big)&\\cdot w_{jk}\\bigg)\\bigg)\\\\\n",
    "&=g_k \\bigg(b_k+\\sum_j \\bigg(g_j\\big(z_j                  \\big)&\\cdot w_{jk}\\bigg)\\bigg)\\\\\n",
    "&=g_k \\bigg(b_k+\\sum_j \\bigg(a_j                               &\\cdot w_{jk}\\bigg)\\bigg)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "b_k+\\sum_j \\left(a_j \\cdot w_{jk}\\right)\n",
    "= \\underbrace{b_k}_{\\text{biases['b_1']}}\n",
    "+\n",
    "\\underbrace{\\sum_j \\left(a_j \\cdot w_{jk}\\right)}_{\\text{network_input @ weights['w_1']}}\n",
    "$$\n",
    "\n",
    "equivalents in a single-layer network to\n",
    "\n",
    "```python\n",
    "z_output = network_input @ weights['w_1'] + biases['b_1']\n",
    "```\n",
    "\n",
    "And\n",
    "$$\n",
    "a_k\n",
    "= g_k \\left( b_k+\\sum_j \\left(a_j \\cdot w_{jk}\\right) \\right)\n",
    "= \\underbrace{g_k}_{\\text{ g_activation['g_out'] }}\n",
    "\\bigg(\n",
    "\\underbrace{ b_k+\\sum_j \\left(a_j \\cdot w_{jk} \\right)}_{\\text{z_output}}\n",
    "\\bigg)\n",
    "$$\n",
    "\n",
    "equivalents\n",
    "\n",
    "```python\n",
    "a_output = g_activation['g_out'](z_output) # Network prediction\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df749f38",
   "metadata": {},
   "source": [
    "- In our case with `n_hidden_units = 0` (no hidden layer, just 2 inputs and 1 output (and 1 target) for each observation) we had (with $n$ observations) next. Each row of $\\vec{a_k}$ is result of each input (`network_input`) of next loop explained at end of present subsection\n",
    "\n",
    "```python\n",
    "for i, (network_input, target) in enumerate(zip(observations, targets)): \n",
    "    ...\n",
    "    a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(network_input, weights, biases, g_activation)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNN01([2,1], figsize=(3,2), showVals=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9da728",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{a_k}\n",
    "\\,=\\,\n",
    "\\overrightarrow{\n",
    "g_k \\left( b_k+\\sum_j \\left(a_j \\cdot w_{jk}\\right) \\right)\n",
    "}\n",
    "\\,=\\,\n",
    "g_k\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{ccc}\n",
    "        a^{(0)}_{1_{1}}\n",
    "        &a^{(0)}_{2_{1}}\n",
    "        &1\n",
    "        \\\\\n",
    "        a^{(0)}_{1_{2}}\n",
    "        &a^{(0)}_{2_{2}}\n",
    "        &1\n",
    "        \\\\\n",
    "        \\vdots\n",
    "        &\\vdots\n",
    "        &\\vdots\n",
    "        \\\\\n",
    "        a^{(0)}_{1_{n}}\n",
    "        &a^{(0)}_{2_{n}}\n",
    "        &1\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "    \\,\\times\\,\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "        \\omega_{1-1}^{(1)}\n",
    "        \\\\\n",
    "        \\omega_{1-2}^{(1)}\n",
    "        \\\\\n",
    "        b_1^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\right)\n",
    "\\,=\\,\n",
    "g_k\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "     a_{1_{1}}^{(0)}\\omega_{1-1}^{(1)} \\,+\\, a_{2_{1}}^{(0)}\\omega_{1-2}^{(1)} \\,+\\, b_1^{(1)}\n",
    "    \\\\\n",
    "     a_{1_{2}}^{(0)}\\omega_{1-1}^{(1)} \\,+\\, a_{2_{2}}^{(0)}\\omega_{1-2}^{(1)} \\,+\\, b_1^{(1)}\n",
    "    \\\\\n",
    "     \\vdots\n",
    "    \\\\\n",
    "     a_{1_{n}}^{(0)}\\omega_{1-1}^{(1)} \\,+\\, a_{2_{n}}^{(0)}\\omega_{1-2}^{(1)} \\,+\\, b_1^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\right)\n",
    "\\,=\\,\n",
    "g_k\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "     z_{1_{1}}^{(1)}\n",
    "    \\\\\n",
    "     z_{1_{2}}^{(1)}\n",
    "    \\\\\n",
    "     \\vdots\n",
    "    \\\\\n",
    "     z_{1_{n}}^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\right)\n",
    "\\,=\\,\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "     a_{1_{1}}^{(1)}\n",
    "    \\\\\n",
    "     a_{1_{2}}^{(1)}\n",
    "    \\\\\n",
    "     \\vdots\n",
    "    \\\\\n",
    "     a_{1_{n}}^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fdb34e",
   "metadata": {},
   "source": [
    "- with `n_hidden_units = 1` and that hidden layer with 4 cells, then we had (for example with $n$ observations of 2 cells each $(a^{(0)}_{1_{1}},a^{(0)}_{2_{1}})$ to $(a^{(0)}_{1_{n}},a^{(0)}_{2_{n}})$):\n",
    "    - 1st calculate $\\vec{z_j}$ and $\\vec{a_j}$ ($z$ and $output$ of hidden layer 1 (one and only hidden layer here)\n",
    "        ```python\n",
    "        z_hidden = network_input @ weights['w_1'] + biases['b_1']\n",
    "        a_hidden = g_activation['g_1'](z_hidden)\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e73bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNN01_2_4_1 = plotNN01([2,4,1], figsize=(8,3), showVals=False, radialBool=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2cbb6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ll}\n",
    "&\\vec{a_j}\n",
    "\\,=\\,\n",
    "\\overrightarrow{\n",
    "g_j \\left( b_j+\\sum_i \\left(a_i \\cdot w_{ij}\\right) \\right)\n",
    "}\n",
    "\\,=\\,\n",
    "g_j\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{ccc}\n",
    "        a^{(0)}_{1_{1}}\n",
    "        &a^{(0)}_{2_{1}}\n",
    "        &1\n",
    "        \\\\\n",
    "        a^{(0)}_{1_{2}}\n",
    "        &a^{(0)}_{2_{2}}\n",
    "        &1\n",
    "        \\\\\n",
    "        \\vdots\n",
    "        &\\vdots\n",
    "        &\\vdots\n",
    "        \\\\\n",
    "        a^{(0)}_{1_{n}}\n",
    "        &a^{(0)}_{2_{n}}\n",
    "        &1\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "    \\,\\times\\,\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "        \\omega_{1-1}^{(1)}\n",
    "        &\\omega_{2-1}^{(1)}\n",
    "        &\\omega_{3-1}^{(1)}\n",
    "        &\\omega_{4-1}^{(1)}\n",
    "        \\\\\n",
    "        \\omega_{1-2}^{(1)}\n",
    "        &\\omega_{2-2}^{(1)}\n",
    "        &\\omega_{3-2}^{(1)}\n",
    "        &\\omega_{4-2}^{(1)}\n",
    "        \\\\\n",
    "        b_1^{(1)}\n",
    "        &b_2^{(1)}\n",
    "        &b_3^{(1)}\n",
    "        &b_4^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\right)\n",
    "\\\\\n",
    "&\\,=\\,\n",
    "g_j\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "     a_{1_{1}}^{(0)}\\omega_{1-1}^{(1)} \\,+\\, a_{2_{1}}^{(0)}\\omega_{1-2}^{(1)} \\,+\\, b_1^{(1)} \n",
    "    &a_{1_{1}}^{(0)}\\omega_{2-1}^{(1)} \\,+\\, a_{2_{1}}^{(0)}\\omega_{2-2}^{(1)} \\,+\\, b_2^{(1)}\n",
    "    &a_{1_{1}}^{(0)}\\omega_{3-1}^{(1)} \\,+\\, a_{2_{1}}^{(0)}\\omega_{3-2}^{(1)} \\,+\\, b_3^{(1)}\n",
    "    &a_{1_{1}}^{(0)}\\omega_{4-1}^{(1)} \\,+\\, a_{2_{1}}^{(0)}\\omega_{4-2}^{(1)} \\,+\\, b_4^{(1)}\n",
    "    \\\\\n",
    "     a_{1_{2}}^{(0)}\\omega_{1-1}^{(1)} \\,+\\, a_{2_{2}}^{(0)}\\omega_{1-2}^{(1)} \\,+\\, b_1^{(1)} \n",
    "    &a_{1_{2}}^{(0)}\\omega_{2-1}^{(1)} \\,+\\, a_{2_{2}}^{(0)}\\omega_{2-2}^{(1)} \\,+\\, b_2^{(1)}\n",
    "    &a_{1_{2}}^{(0)}\\omega_{3-1}^{(1)} \\,+\\, a_{2_{2}}^{(0)}\\omega_{3-2}^{(1)} \\,+\\, b_3^{(1)}\n",
    "    &a_{1_{2}}^{(0)}\\omega_{4-1}^{(1)} \\,+\\, a_{2_{2}}^{(0)}\\omega_{4-2}^{(1)} \\,+\\, b_4^{(1)}\n",
    "    \\\\\n",
    "     \\vdots \n",
    "    &\\vdots\n",
    "    &\\vdots\n",
    "    &\\vdots\n",
    "    \\\\\n",
    "     a_{1_{n}}^{(0)}\\omega_{1-1}^{(1)} \\,+\\, a_{2_{n}}^{(0)}\\omega_{1-2}^{(1)} \\,+\\, b_1^{(1)} \n",
    "    &a_{1_{n}}^{(0)}\\omega_{2-1}^{(1)} \\,+\\, a_{2_{n}}^{(0)}\\omega_{2-2}^{(1)} \\,+\\, b_2^{(1)}\n",
    "    &a_{1_{n}}^{(0)}\\omega_{3-1}^{(1)} \\,+\\, a_{2_{n}}^{(0)}\\omega_{3-2}^{(1)} \\,+\\, b_3^{(1)}\n",
    "    &a_{1_{n}}^{(0)}\\omega_{4-1}^{(1)} \\,+\\, a_{2_{n}}^{(0)}\\omega_{4-2}^{(1)} \\,+\\, b_4^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\right)\n",
    "\\\\\n",
    "&\\,=\\,\n",
    "g_j\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "     z_{1_{1}}^{(1)} \n",
    "    &z_{2_{1}}^{(1)}\n",
    "    &z_{3_{1}}^{(1)}\n",
    "    &z_{4_{1}}^{(1)}\n",
    "    \\\\\n",
    "     z_{1_{2}}^{(1)} \n",
    "    &z_{2_{2}}^{(1)}\n",
    "    &z_{3_{2}}^{(1)}\n",
    "    &z_{4_{2}}^{(1)}\n",
    "    \\\\\n",
    "     \\vdots \n",
    "    &\\vdots\n",
    "    &\\vdots\n",
    "    &\\vdots\n",
    "    \\\\\n",
    "     z_{1_{n}}^{(1)} \n",
    "    &z_{2_{n}}^{(1)}\n",
    "    &z_{3_{n}}^{(1)}\n",
    "    &z_{4_{n}}^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\right)\n",
    "\\,=\\,\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "     a_{1_{1}}^{(1)} \n",
    "    &a_{2_{1}}^{(1)}\n",
    "    &a_{3_{1}}^{(1)}\n",
    "    &a_{4_{1}}^{(1)}\n",
    "    \\\\\n",
    "     a_{1_{2}}^{(1)} \n",
    "    &a_{2_{2}}^{(1)}\n",
    "    &a_{3_{2}}^{(1)}\n",
    "    &a_{4_{2}}^{(1)}\n",
    "    \\\\\n",
    "     \\vdots \n",
    "    &\\vdots\n",
    "    &\\vdots\n",
    "    &\\vdots\n",
    "    \\\\\n",
    "     a_{1_{n}}^{(1)} \n",
    "    &a_{2_{n}}^{(1)}\n",
    "    &a_{3_{n}}^{(1)}\n",
    "    &a_{4_{n}}^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Note this LaTeX notation subindixes refere first to target layer then to origin layer, so for example $\\omega_{4-2}^{(1)}$ actually represents $\\omega_{i_2j_4}$ of Python plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e45c9",
   "metadata": {},
   "source": [
    "- with `n_hidden_units = 4` (4 neurons in unique hidden layer) ...\n",
    "    - 2nd calculate $\\vec{z_k}$ and $\\vec{a_k}$\n",
    "    ```python\n",
    "    z_output = a_hidden @ weights['w_2'] + biases['b_2']\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0524c5c",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{ll}\n",
    "\\vec{a_k}\n",
    "\\,=\\,\n",
    "\\overrightarrow{\n",
    "g_k \\left( b_k+\\sum_j \\left(a_j \\cdot w_{jk}\\right) \\right)\n",
    "}\n",
    "\\,=\\,\n",
    "g_k\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "         a_{1_{1}}^{(1)} \n",
    "        &a_{2_{1}}^{(1)}\n",
    "        &a_{3_{1}}^{(1)}\n",
    "        &a_{4_{1}}^{(1)}\n",
    "        &1\n",
    "        \\\\\n",
    "         a_{1_{2}}^{(1)} \n",
    "        &a_{2_{2}}^{(1)}\n",
    "        &a_{3_{2}}^{(1)}\n",
    "        &a_{4_{2}}^{(1)}\n",
    "        &1\n",
    "        \\\\\n",
    "         \\vdots \n",
    "        &\\vdots \n",
    "        &\\vdots \n",
    "        &\\vdots \n",
    "        &\\vdots \n",
    "        \\\\\n",
    "         a_{1_{n}}^{(1)} \n",
    "        &a_{2_{n}}^{(1)}\n",
    "        &a_{3_{n}}^{(1)}\n",
    "        &a_{4_{n}}^{(1)}\n",
    "        &1\n",
    "    \\end{array}\n",
    "    \\right] \n",
    "    \\,\\cdot\\,\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "        \\omega_{1-1}^{(2)} \\\\\n",
    "        \\omega_{1-2}^{(2)} \\\\\n",
    "        \\omega_{1-3}^{(2)} \\\\\n",
    "        \\omega_{1-4}^{(2)} \\\\\n",
    "        b_1^{(2)}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\right)\n",
    "\\,=\\,\n",
    "g_k\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "             a_{1_{1}}^{(1)} \\omega_{1-1}^{(2)} \n",
    "        \\,+\\,a_{2_{1}}^{(1)} \\omega_{1-2}^{(2)}\n",
    "        \\,+\\,a_{3_{1}}^{(1)} \\omega_{1-3}^{(2)}\n",
    "        \\,+\\,a_{4_{1}}^{(1)} \\omega_{1-4}^{(2)}\n",
    "        \\,+\\,b_1^{(2)}\n",
    "        \\\\\n",
    "             a_{1_{2}}^{(1)} \\omega_{1-1}^{(2)} \n",
    "        \\,+\\,a_{2_{2}}^{(1)} \\omega_{1-2}^{(2)}\n",
    "        \\,+\\,a_{3_{2}}^{(1)} \\omega_{1-3}^{(2)}\n",
    "        \\,+\\,a_{4_{2}}^{(1)} \\omega_{1-4}^{(2)}\n",
    "        \\,+\\,b_1^{(2)}\n",
    "        \\\\\n",
    "         \\vdots \n",
    "        \\\\\n",
    "             a_{1_{n}}^{(1)} \\omega_{1-1}^{(2)}\n",
    "        \\,+\\,a_{2_{n}}^{(1)} \\omega_{1-2}^{(2)}\n",
    "        \\,+\\,a_{3_{n}}^{(1)} \\omega_{1-3}^{(2)}\n",
    "        \\,+\\,a_{4_{n}}^{(1)} \\omega_{1-4}^{(2)}\n",
    "        \\,+\\,b_1^{(2)}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\right)\n",
    "\\,=\\,\n",
    "g_k\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "     z_{1_{1}}^{(2)} \\\\\n",
    "     z_{1_{2}}^{(2)} \\\\\n",
    "     \\vdots  \\\\\n",
    "     z_{1_{n}}^{(2)} \n",
    "    \\end{array}\n",
    "    \\right]        \n",
    "\\right)\n",
    "\\,=\\,\n",
    "\\left[\n",
    "    \\begin{array}{cccc}\n",
    "     a_{1_{1}}^{(2)} \\\\\n",
    "     a_{1_{2}}^{(2)} \\\\\n",
    "     \\vdots  \\\\\n",
    "     a_{1_{n}}^{(2)} \n",
    "    \\end{array}\n",
    "\\right]\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e105c9bb",
   "metadata": {},
   "source": [
    "-------------\n",
    "As we will see in [#Whole-function](#Whole-function) section, the `run_ann_training_simulation` loops `n_iterations`, and enclosed the loop of each `(network_input, target)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974816e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some args of run_ann_training_simulation\n",
    "n_iterations=2\n",
    "learning_rate=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1899e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    obs_error = []\n",
    "    print(f\"iteration: {iteration}\")\n",
    "    for i, (network_input, target) in enumerate(zip(observations, targets)):\n",
    "        network_input = np.atleast_2d(network_input)\n",
    "        \n",
    "        # Step I: Forward propagate input signal through the network,\n",
    "        # collecting activations and hidden states\n",
    "        a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(\n",
    "            network_input, weights, biases, g_activation\n",
    "        )\n",
    "        \n",
    "        if i==iteration: # random\n",
    "            print(f\"i: {i}\")\n",
    "            display_htmlFcn2(\"network_input\", \"weights['w_1']\",\n",
    "                 \"network_input @ weights['w_1']\",\n",
    "                 \"biases['b_1']\", \"network_input @ weights['w_1'] + biases['b_1']\",\n",
    "                 \"g_activation['g_out'](z_output)\", # a_output\n",
    "                 \"(lambda z: 1./(1. + np.exp(-z)))(network_input @ weights['w_1'] + biases['b_1'])\", # a_output\n",
    "                 _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=2)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5326d49",
   "metadata": {},
   "source": [
    "#### Step II code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3b611",
   "metadata": {},
   "source": [
    "**Step II: back-propagate error signals**\n",
    "\n",
    "Calculate the network output error and backpropagate it toward the input. For this walkthrough we’ll continue to use the sum of squared differences error function, this time written in a more explicit form than in the [Training neural networks & gradient descent](#Training-neural-networks-&-gradient-descent) section:\n",
    "\n",
    "$$\n",
    "\\begin{align}E(\\mathbf{\\theta}) &= \\frac{1}{2}\\sum_{k \\in K}(a_k - t_k)^2\\end{align}\n",
    "$$\n",
    "\n",
    "Here we sum over the values of all $k$ output units (one in this example). Note that the model parameters parameters $\\theta$ are implicit in the output activations $a_k$. This error function has the following derivative with respect to the model parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "E'(\\mathbf{\\theta}) = (a_k - t_k)\n",
    "$$\n",
    "\n",
    "We can now define an “error signal” $\\theta_k$ at the **output node** that will be backpropagated toward the input. The error signal is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_k &= g_k'(z_k)E'(\\theta) \\\\  \n",
    "&= g_k'(z_k)(a_k - t_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This error signal essentially weights the gradient of the error function by the gradient of the output activation function. Notice that there is a $z_k$ term is used in the calculation of $\\delta_k$. In order to make learning more efficient, we keep track of the $z_k$ during the forward-propagation step so that it can be used in backpropagation, notice `z_output` as 2nd output of:\n",
    "```python\n",
    "a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(network_input, weights, biases, g_activation)\n",
    "```\n",
    "\n",
    "We can continue backpropagating the error signal toward the input by passing $\\delta_k$ through the output layer weights $w_{jk}$, summing over all output nodes, and passing the result through the gradient of the activation function at the **hidden layer** $g_j'(z_j)$ (Figure 6-II). Performing these operations results in the backpropagated error signal for the hidden layer, $\\delta_j$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta_j = g_j'(z_j)\\sum_k \\delta_k w_{jk}\n",
    "$$\n",
    "\n",
    "For networks that have more than one hidden layer, this error backpropagation procedure can continue for layers $j−1$,$j−2$,..., etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5a5f5",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "- output layer $k$ gives `delta_output`:\n",
    "$$\n",
    "\\delta_k  = g_k'(z_k)(a_k - t_k)\n",
    "$$\n",
    "\n",
    "- hidden layer $j$ and direct forward layer $k$ gives `delta_hidden` (of 1st hidden layer):\n",
    "$$\n",
    "\\delta_j = g_j'(z_j)\\sum_k \\delta_k w_{jk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94135d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_II_backprop(target, a_output, z_output, z_hidden, weights, g_activation_prime):\n",
    "    # Calculate error function derivative given input/output/params\n",
    "    delta_output = g_activation_prime['g_out'](z_output) * (a_output - target)\n",
    "\n",
    "    # Calculate any error contributions from hidden layers nodes\n",
    "    if 'w_2' in weights:  # multi-layer network\n",
    "        delta_hidden = g_activation_prime['g_1'](z_hidden) * (delta_output @ weights['w_2'].T)\n",
    "    else:\n",
    "        delta_hidden = np.array([])\n",
    "    return delta_output, delta_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea8322",
   "metadata": {},
   "source": [
    "Next is for $n$ observations and just 1 output neuron. \n",
    "Each row of $\\vec{\\delta_k}$ is result of each input-target (`(network_input, target)`) of next loop explained at end of present subsection\n",
    "\n",
    "```python\n",
    "for i, (network_input, target) in enumerate(zip(observations, targets)): \n",
    "    ...\n",
    "    a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(network_input, weights, biases, g_activation)\n",
    "    delta_output, delta_hidden = step_II_backprop(target, a_output, z_output, z_hidden, weights, g_activation_prime)\n",
    "    ...\n",
    "```\n",
    "\n",
    "$$\n",
    "\\vec{\\delta_k}\n",
    "\\,=\\, \\overrightarrow{error} \\cdot \\overrightarrow{g_k'(z_k)}\n",
    "\\,=\\,\n",
    "    \\left(\n",
    "        \\left[\n",
    "        \\begin{array}{ccc}\n",
    "            t_1\\\\\n",
    "            t_2\\\\\n",
    "            \\vdots\\\\\n",
    "            t_n\\\\\n",
    "        \\end{array}\n",
    "        \\right] \n",
    "        \\,-\\,\n",
    "        \\left[\n",
    "        \\begin{array}{cccc}\n",
    "         a_{1_{1}}^{(l)} \n",
    "        \\\\\n",
    "         a_{1_{2}}^{(l)} \n",
    "        \\\\\n",
    "         \\vdots\n",
    "        \\\\\n",
    "         a_{1_{n}}^{(l)} \n",
    "        \\end{array}\n",
    "        \\right]\n",
    "    \\right)\n",
    "    \\,\\cdot\\,\n",
    "    g_k'\n",
    "    \\left(\n",
    "        \\left[\n",
    "        \\begin{array}{cccc}\n",
    "         z_{1_{1}}^{(l)} \n",
    "        \\\\\n",
    "         z_{1_{2}}^{(l)} \n",
    "        \\\\\n",
    "         \\vdots \n",
    "        \\\\\n",
    "         z_{1_{n}}^{(l)} \n",
    "        \\end{array}\n",
    "        \\right]\n",
    "    \\right)\n",
    "\\,=\\,\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    (t_1 - a_{1_{1}}^{(l)}) \\cdot g_k'\\left(z_{1_{1}}^{(l)}\\right)\\\\\n",
    "    (t_2 - a_{1_{2}}^{(l)}) \\cdot g_k'\\left(z_{1_{2}}^{(l)}\\right)\\\\\n",
    "    \\vdots \\\\\n",
    "    (t_n - a_{1_{n}}^{(l)}) \\cdot g_k'\\left(z_{1_{n}}^{(l)}\\right)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Replace each variable of style $x^{(l)}$ with $x^{(1)}$ if NN has no hidden layer ($l$ from layer), or with $x^{(2)}$ if 1 hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0928b70",
   "metadata": {},
   "source": [
    "In Python\n",
    "\n",
    "$$\n",
    "\\delta_k  = \n",
    "\\underbrace{\n",
    "g_k'(z_k)\n",
    "}_{\\text{g_activation_prime['g_out'](z_output)}}\n",
    "\\underbrace{\n",
    "(a_k - t_k)\n",
    "}_{\\text{(a_output - target)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c49971",
   "metadata": {},
   "source": [
    "$\\bullet$ With `n_hidden_units = 0` (no hidden layer), $\\vec{\\delta_j}$ does not exist:\n",
    "\n",
    "$$\n",
    "\\delta_j = \n",
    "\\underbrace{\n",
    "g_j'(z_j)\\sum_k \\delta_k w_{jk}\n",
    "}_{\\text{np.array([])}}\n",
    "$$\n",
    "\n",
    "$\\bullet$ If the hidden layer exists (with `n_hidden_units`=4 neurons), then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNN01_2_4_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692239a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{\\delta_j} = \n",
    "\\overrightarrow{\n",
    "g_j'(z_j)\\sum_k \\delta_k w_{jk}\n",
    "}\n",
    "=\n",
    "g_j'\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "      z_{1_{1}}^{(1)}\n",
    "     &z_{2_{1}}^{(1)}\n",
    "     &z_{3_{1}}^{(1)}\n",
    "     &z_{4_{1}}^{(1)}\\\\\n",
    "      z_{1_{2}}^{(1)}\n",
    "     &z_{2_{2}}^{(1)}\n",
    "     &z_{3_{2}}^{(1)}\n",
    "     &z_{4_{2}}^{(1)}\\\\\n",
    "      \\vdots\n",
    "     &\\vdots\n",
    "     &\\vdots\n",
    "     &\\vdots\\\\\n",
    "      z_{1_{n}}^{(1)}\n",
    "     &z_{2_{n}}^{(1)}\n",
    "     &z_{3_{n}}^{(1)}\n",
    "     &z_{4_{n}}^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    (t_1 - a_{1_{1}}^{(2)}) \\cdot g_k'\\left(z_{1_{1}}^{(2)}\\right)\\\\\n",
    "    (t_2 - a_{1_{2}}^{(2)}) \\cdot g_k'\\left(z_{1_{2}}^{(2)}\\right)\\\\\n",
    "    \\vdots \\\\\n",
    "    (t_n - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\times\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    \\omega_{1-1}^{(2)}\\\\\n",
    "    \\omega_{1-2}^{(2)}\\\\\n",
    "    \\omega_{1-3}^{(2)}\\\\\n",
    "    \\omega_{1-4}^{(2)}\n",
    "\\end{array}\n",
    "\\right]^T\n",
    "=\n",
    "g_j'\n",
    "\\left(\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "      z_{1_{1}}^{(1)}\n",
    "     &z_{2_{1}}^{(1)}\n",
    "     &z_{3_{1}}^{(1)}\n",
    "     &z_{4_{1}}^{(1)}\\\\\n",
    "      z_{1_{2}}^{(1)}\n",
    "     &z_{2_{2}}^{(1)}\n",
    "     &z_{3_{2}}^{(1)}\n",
    "     &z_{4_{2}}^{(1)}\\\\\n",
    "      \\vdots\n",
    "     &\\vdots\n",
    "     &\\vdots\n",
    "     &\\vdots\\\\\n",
    "      z_{1_{n}}^{(1)}\n",
    "     &z_{2_{n}}^{(1)}\n",
    "     &z_{3_{n}}^{(1)}\n",
    "     &z_{4_{n}}^{(1)}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\right)\n",
    "\\cdot\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "  (t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    " &(t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    " &(t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    " &(t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\\\\\n",
    "  (t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    " &(t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    " &(t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    " &(t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\\\\\n",
    "  \\vdots\n",
    " &\\vdots\n",
    " &\\vdots\n",
    " &\\vdots\\\\\n",
    "  (t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    " &(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    " &(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    " &(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "  g_j'(z_{1_{1}}^{(1)})(t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    " &g_j'(z_{2_{1}}^{(1)})(t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    " &g_j'(z_{3_{1}}^{(1)})(t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    " &g_j'(z_{4_{1}}^{(1)})(t_1 - a_{1_{1}}^{(2)})\\cdot g_k'(z_{1_{1}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\\\\\n",
    "  g_j'(z_{1_{2}}^{(1)})(t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    " &g_j'(z_{2_{2}}^{(1)})(t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    " &g_j'(z_{3_{2}}^{(1)})(t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    " &g_j'(z_{4_{2}}^{(1)})(t_2 - a_{1_{2}}^{(2)})\\cdot g_k'(z_{1_{2}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\\\\\n",
    "  \\vdots\n",
    " &\\vdots\n",
    " &\\vdots\n",
    " &\\vdots\\\\\n",
    "  g_j'(z_{1_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    " &g_j'(z_{2_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    " &g_j'(z_{3_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    " &g_j'(z_{4_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ded5a",
   "metadata": {},
   "source": [
    "Notice that for just 1 output neuron $\\vec{\\delta_k}$ is 1D array, thus the delta of each neuron $j$ of unique hidden layer can be simplified to:\n",
    "$$\n",
    "\\vec{\\delta_j}\n",
    "= \n",
    "\\overrightarrow{\n",
    "g_j'(z_j)\\sum_k \\delta_k w_{jk}\n",
    "}\n",
    "= \n",
    "\\overrightarrow{\n",
    "g_j'(z_j)\\delta_k w_{jk}\n",
    "}\n",
    "$$\n",
    "\n",
    "for example neuron $j_{1}$:\n",
    "$\\vec{\\delta_{j_{1}}}=g_j'(z_{j_{1}})\\delta_k w_{j_{1}k_{1}}$\n",
    "which values in $n$-th observation using matrix-notation \n",
    "$g_j'(z_{1_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-1}^{(2)}$\n",
    "\n",
    "If we had 2 output neurons then $\\vec{\\delta_k}$ and therefore calculus of $\\vec{\\delta_j}$ were harder.\n",
    "\n",
    "$$\n",
    "\\vec{\\delta_k}_{\\text{2 neurons}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "     (t_{1_{1}} - a_{1_{1}}^{(2)}) \\cdot g_k'\\left(z_{1_{1}}^{(2)}\\right)\n",
    "    &(t_{2_{1}} - a_{2_{1}}^{(2)}) \\cdot g_k'\\left(z_{2_{1}}^{(2)}\\right)\\\\\n",
    "     (t_{1_{2}} - a_{1_{2}}^{(2)}) \\cdot g_k'\\left(z_{1_{2}}^{(2)}\\right)\n",
    "    &(t_{2_{2}} - a_{2_{2}}^{(2)}) \\cdot g_k'\\left(z_{2_{2}}^{(2)}\\right)\\\\\n",
    "    \\vdots \n",
    "    &\\vdots\\\\    \n",
    "     (t_{1_{n}} - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\n",
    "    &(t_{2_{n}} - a_{2_{n}}^{(2)}) \\cdot g_k'\\left(z_{2_{n}}^{(2)}\\right)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d32205",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    obs_error = []\n",
    "    print(f\"iteration: {iteration}\")\n",
    "    for i, (network_input, target) in enumerate(zip(observations, targets)):\n",
    "        network_input = np.atleast_2d(network_input)\n",
    "        \n",
    "        # Step I: Forward propagate input signal through the network,\n",
    "        # collecting activations and hidden states\n",
    "        a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(\n",
    "            network_input, weights, biases, g_activation\n",
    "        )\n",
    "        \n",
    "        # Step II: Backpropagate error signal\n",
    "        delta_output, delta_hidden = step_II_backprop(\n",
    "            target, a_output, z_output, z_hidden, weights, g_activation_prime\n",
    "        )\n",
    "        if i==iteration: # random\n",
    "            print(f\"i: {i}\")\n",
    "            display_htmlFcn2(\"a_output\", \"[target]\", \n",
    "                             \"a_output - target\",\n",
    "                             \"z_output\",\n",
    "                             \"g_activation_prime['g_out'](z_output)\",\n",
    "                             \"g_activation_prime['g_out'](z_output) * (a_output - target)\", # delta_output\n",
    "                             _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=[2,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2975d39",
   "metadata": {},
   "source": [
    "#### Step III code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae4142",
   "metadata": {},
   "source": [
    "**Step III: calculate parameter gradients**\n",
    "\n",
    "Calculate the gradients of the error function with respect to the model parameters at each layer $l$ using the forward signals $a_{l−1}$, and the backward error signals $\\delta_l$ . If one considers the model weights $w_{l−1,l}$ at a layer $l$ as linking the forward signal $a_{l−1}$ to the error signal $\\delta_l$ (Figure 6-III), then the gradient of the error function with respect to those weights is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{l-1, l}} = a_{l-1}\\delta_l\n",
    "$$\n",
    "\n",
    "Thus the gradient of the error function with respect to the model weight at each layer can be efficiently calculated by simply keeping track of the forward-propagated activations feeding into that layer from below, and weighting those activations by the backward-propagated error signals feeding into that layer from above!\n",
    "\n",
    "What about the bias parameters? It turns out that the same gradient rule used for the weights applies, except that “feed-forward activations” for biases are always +1 (see Figure 1). Thus the bias gradients for layer $l$\n",
    "are simply:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial b_{l}} = (1)\\delta_l = \\delta_l\n",
    "$$\n",
    "\n",
    "Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2667c7d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{l-1, l}} = a_{l-1}\\delta_l\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial b_{l}} = (1)\\delta_l = \\delta_l\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada60d0a",
   "metadata": {},
   "source": [
    "$\\bullet$ With `n_hidden_units = 0`. For $n$ observations. \n",
    "Each row of $\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial w_{l-1, l}} }$ and $\\overrightarrow{\\frac{\\partial E}{\\partial b_{l}}}$ is result of each input-target (`(network_input, target)`) of next loop explained at end of present subsection\n",
    "\n",
    "```python\n",
    "for i, (network_input, target) in enumerate(zip(observations, targets)): \n",
    "    ...\n",
    "    a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(network_input, weights, biases, g_activation)\n",
    "    delta_output, delta_hidden = step_II_backprop(target, a_output, z_output, z_hidden, weights, g_activation_prime)\n",
    "    weight_gradients, bias_gradients = step_III_gradient_calculation(delta_output, delta_hidden, a_hidden, network_input, weight_gradients, bias_gradients)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a96d91",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial w_{l-1, l}} \n",
    "}\n",
    "= \\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial w_{j k}}\n",
    "}\n",
    "= \\vec{a_{j}}\\cdot\\vec{\\delta_k}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "      a^{(0)}_{1_{1}} \n",
    "     &a^{(0)}_{2_{1}}\\\\\n",
    "      a^{(0)}_{1_{2}}  \n",
    "     &a^{(0)}_{2_{2}}\\\\\n",
    "      \\vdots\n",
    "     &\\vdots\\\\\n",
    "      a^{(0)}_{1_{n}} \n",
    "     &a^{(0)}_{2_{n}} \n",
    "\\end{array}\n",
    "\\right]^T\n",
    "\\cdot\n",
    "\\vec{\\delta_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial b_{l}}\n",
    "}\n",
    "=\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial b_{k}}\n",
    "}\n",
    "= \\vec{\\delta_k}\n",
    "$$\n",
    "\n",
    "where $\\vec{\\delta_k}$ was calculated in step II."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f2569",
   "metadata": {},
   "source": [
    "$\\bullet$ With `n_hidden_units = 0` (no hidden layer). For example weight and bias gradients of of obsevation $n$ (no hidden layer and just 1 ouput):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6f12e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial w_{{l-1, l}_{n}}} \n",
    "}\n",
    "= \\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial w_{{j k}_{n}}}\n",
    "}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "     \\frac{\\partial E}{\\partial w_{j_{1}k_{1}}}_{n}\\\\\n",
    "     \\frac{\\partial E}{\\partial w_{j_{2}k_{1}}}_{n} \n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\vec{a_{j_{n}}}\\cdot\\vec{\\delta_{k_{n}}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "          a^{(0)}_{1_{n}}\n",
    "         &a^{(0)}_{2_{n}} \n",
    "    \\end{array}\n",
    "    \\right]^T\n",
    "}_{\\text{network_input.T}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    (t_n - a_{1_{n}}^{(1)}) \\cdot g_k'\\left(z_{1_{n}}^{(1)}\\right)\n",
    "    \\right]\n",
    "}_{\\text{delta_output}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "      a^{(0)}_{1_{n}}(t_n - a_{1_{n}}^{(1)}) \\cdot g_k'\\left(z_{1_{n}}^{(1)}\\right)\\\\\n",
    "      a^{(0)}_{2_{n}}(t_n - a_{1_{n}}^{(1)}) \\cdot g_k'\\left(z_{1_{n}}^{(1)}\\right)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial b_{l_{n}}}\n",
    "}\n",
    "=\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial b_{k_{n}}}\n",
    "}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "     \\frac{\\partial E}{\\partial b_{k1}}_{n} \n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\vec{\\delta_{k_{n}}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "      (t_n - a_{1_{n}}^{(1)}) \\cdot g_k'\\left(z_{1_{n}}^{(1)}\\right)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcff5e8",
   "metadata": {},
   "source": [
    "$\\bullet$ If the hidden layer exists (with `n_hidden_units`=4 neurons), then $\\vec{\\delta_k}$ and $\\vec{\\delta_j}$ are as complex as showed in step II. Thus,\n",
    " - the biases gradients are those delta arrays, while \n",
    " - the weights gradients are those $\\delta$ multiplied respect. by $\\vec{a_k}$ or $\\vec{a_j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNN01_2_4_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253fec3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underbrace{\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial w_{{j k}_{n}}}\n",
    "}\n",
    "}_{\\text{weight_gradients['w_2']}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "     \\frac{\\partial E}{\\partial w_{j_{1}k_{1}}}_{n}\\\\\n",
    "     \\frac{\\partial E}{\\partial w_{j_{2}k_{1}}}_{n}\\\\\n",
    "     \\frac{\\partial E}{\\partial w_{j_{3}k_{1}}}_{n}\\\\\n",
    "     \\frac{\\partial E}{\\partial w_{j_{4}k_{1}}}_{n}  \n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\vec{a_{j_{n}}}\\cdot\\vec{\\delta_{k_{n}}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "          a^{(1)}_{1_{n}}\n",
    "         &a^{(1)}_{2_{n}} \n",
    "         &a^{(1)}_{3_{n}} \n",
    "         &a^{(1)}_{4_{n}} \n",
    "    \\end{array}\n",
    "    \\right]^T\n",
    "}_{\\text{a_hidden.T}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    (t_n - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\n",
    "    \\right]\n",
    "}_{\\text{delta_output}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "      a^{(1)}_{1_{n}}(t_n - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\\\\\n",
    "      a^{(1)}_{2_{n}}(t_n - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\\\\\n",
    "      a^{(1)}_{3_{n}}(t_n - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\\\\\n",
    "      a^{(1)}_{4_{n}}(t_n - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial b_{k_{n}}}\n",
    "}\n",
    "}_{\\text{bias_gradients['b_2']}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "     \\frac{\\partial E}{\\partial b_{k1}}_{n} \n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\vec{\\delta_{k_{n}}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    (t_n - a_{1_{n}}^{(2)}) \\cdot g_k'\\left(z_{1_{n}}^{(2)}\\right)\n",
    "    \\right]\n",
    "}_{\\text{delta_output}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd3107",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underbrace{\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial w_{{i j}_{n}}}\n",
    "}\n",
    "}_{\\text{weight_gradients['w_1']}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "      \\frac{\\partial E}{\\partial w_{i_{1}j_{1}}}_{n}\n",
    "     &\\frac{\\partial E}{\\partial w_{i_{1}j_{2}}}_{n}\n",
    "     &\\frac{\\partial E}{\\partial w_{i_{1}j_{3}}}_{n}\n",
    "     &\\frac{\\partial E}{\\partial w_{i_{1}j_{4}}}_{n}\\\\\n",
    "      \\frac{\\partial E}{\\partial w_{i_{2}j_{1}}}_{n}\n",
    "     &\\frac{\\partial E}{\\partial w_{i_{2}j_{2}}}_{n}\n",
    "     &\\frac{\\partial E}{\\partial w_{i_{2}j_{3}}}_{n}\n",
    "     &\\frac{\\partial E}{\\partial w_{i_{2}j_{4}}}_{n}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\vec{a_{i_{n}}}\\cdot\\vec{\\delta_{j_{n}}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "          a^{(0)}_{1_{n}}\n",
    "         &a^{(0)}_{2_{n}} \n",
    "    \\end{array}\n",
    "    \\right]^T\n",
    "}_{\\text{network_input.T}}\n",
    "\\cdot\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "       g_j'(z_{1_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\\\\\n",
    "       g_j'(z_{2_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\\\\\n",
    "       g_j'(z_{3_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\\\\\n",
    "       g_j'(z_{4_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "}_{\\text{delta_hidden}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "      a^{(0)}_{1_{n}} g_j'(z_{1_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    "     &a^{(0)}_{1_{n}} g_j'(z_{2_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    "     &a^{(0)}_{1_{n}} g_j'(z_{3_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    "     &a^{(0)}_{1_{n}} g_j'(z_{4_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\n",
    "     \\\\ \n",
    "      a^{(0)}_{2_{n}} g_j'(z_{1_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\n",
    "     &a^{(0)}_{2_{n}} g_j'(z_{2_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\n",
    "     &a^{(0)}_{2_{n}} g_j'(z_{3_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\n",
    "     &a^{(0)}_{2_{n}} g_j'(z_{4_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\underbrace{\n",
    "\\overrightarrow{\n",
    "\\frac{\\partial E}{\\partial b_{j_{n}}}\n",
    "}\n",
    "}_{\\text{bias_gradients['b_1']}}\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "     \\frac{\\partial E}{\\partial b_{j1}}_{n}\\\\\n",
    "     \\frac{\\partial E}{\\partial b_{j2}}_{n}\\\\\n",
    "     \\frac{\\partial E}{\\partial b_{j3}}_{n}\\\\\n",
    "     \\frac{\\partial E}{\\partial b_{j4}}_{n}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\vec{\\delta_{j_{n}}}\n",
    "=\n",
    "\\underbrace{\n",
    "    \\left[\n",
    "    \\begin{array}{cccc}\n",
    "       g_j'(z_{1_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-1}^{(2)}\\\\\n",
    "       g_j'(z_{2_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-2}^{(2)}\\\\\n",
    "       g_j'(z_{3_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-3}^{(2)}\\\\\n",
    "       g_j'(z_{4_{n}}^{(1)})(t_n - a_{1_{n}}^{(2)})\\cdot g_k'(z_{1_{n}}^{(2)})\\cdot\\omega_{1-4}^{(2)}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "}_{\\text{delta_hidden}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_III_gradient_calculation(\n",
    "    delta_output, delta_hidden, a_hidden, network_input, weight_gradients, bias_gradients\n",
    "):\n",
    "    if 'w_2' in weight_gradients:  # multi-layer network\n",
    "        weight_gradients['w_2'] = a_hidden.T * delta_output\n",
    "        bias_gradients['b_2'] = delta_output * 1\n",
    "        weight_gradients['w_1'] = network_input.T * delta_hidden\n",
    "        bias_gradients['b_1'] = delta_hidden * 1\n",
    "    else:  # single-layer network\n",
    "        weight_gradients['w_1'] = network_input.T * delta_output\n",
    "        bias_gradients['b_1'] = delta_output * 1\n",
    "\n",
    "    return weight_gradients, bias_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    obs_error = []\n",
    "    print(f\"iteration: {iteration}\")\n",
    "    for i, (network_input, target) in enumerate(zip(observations, targets)):\n",
    "        network_input = np.atleast_2d(network_input)\n",
    "        \n",
    "        # Step I: Forward propagate input signal through the network,\n",
    "        # collecting activations and hidden states\n",
    "        a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(\n",
    "            network_input, weights, biases, g_activation\n",
    "        )\n",
    "        \n",
    "        # Step II: Backpropagate error signal\n",
    "        delta_output, delta_hidden = step_II_backprop(\n",
    "            target, a_output, z_output, z_hidden, weights, g_activation_prime\n",
    "        )\n",
    "        \n",
    "        # Step III. Calculate Error gradient w.r.t. parameters\n",
    "        weight_gradients, bias_gradients = step_III_gradient_calculation(\n",
    "            delta_output, delta_hidden, a_hidden, network_input,\n",
    "            weight_gradients, bias_gradients\n",
    "        )\n",
    "        \n",
    "        if i==iteration: # random\n",
    "            print(f\"i: {i}\")\n",
    "            display_htmlFcn2( \n",
    "                             \"g_activation_prime['g_out'](z_output) * (a_output - target)\", # delta_output\n",
    "                             \"bias_gradients['b_1']\", # delta_output\n",
    "                             \"network_input\",\n",
    "                             \"network_input.T * delta_output\", # weight_gradients['w_1']\n",
    "                             _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b80387",
   "metadata": {},
   "source": [
    "#### Step VI code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cc229",
   "metadata": {},
   "source": [
    "**Step IV: update parameters**\n",
    "\n",
    "To update the model parameters based on the gradients calculated in Step III. Note that the gradients point in the direction in parameter space that will increase the value of the error function. Thus when updating the model parameters we should choose to go in the opposite direction. How far do we travel in that direction? That is generally determined by a user-defined step size–aka learning rate–parameter, $\\eta$. Thus, given the parameter gradients and the step size, the weights and biases for a given layer are updated accordingly:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{l-1,l} &\\leftarrow w_{l-1,l} - \\eta \\frac{\\partial E}{\\partial w_{l-1, l}} \\\\ \n",
    "b_l &\\leftarrow b_{l} - \\eta \\frac{\\partial E}{\\partial b_{l}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc43a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_IV_update_parameters(weights, biases, weight_gradients, bias_gradients, learning_rate):\n",
    "    if 'w_2' in weights:  # multi-layer network\n",
    "        weights['w_2'] = weights['w_2'] - weight_gradients['w_2'] * learning_rate\n",
    "        biases['b_2'] = biases['b_2'] - bias_gradients['b_2'] * learning_rate\n",
    "\n",
    "    weights['w_1'] = weights['w_1'] - weight_gradients['w_1'] * learning_rate\n",
    "    biases['b_1'] = biases['b_1'] - bias_gradients['b_1'] * learning_rate\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc67698",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    obs_error = []\n",
    "    print(f\"iteration: {iteration}\")\n",
    "    for i, (network_input, target) in enumerate(zip(observations, targets)):\n",
    "        network_input = np.atleast_2d(network_input)\n",
    "        \n",
    "        # Step I: Forward propagate input signal through the network,\n",
    "        # collecting activations and hidden states\n",
    "        a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(\n",
    "            network_input, weights, biases, g_activation\n",
    "        )\n",
    "        \n",
    "        # Step II: Backpropagate error signal\n",
    "        delta_output, delta_hidden = step_II_backprop(\n",
    "            target, a_output, z_output, z_hidden, weights, g_activation_prime\n",
    "        )\n",
    "        \n",
    "        # Step III. Calculate Error gradient w.r.t. parameters\n",
    "        weight_gradients, bias_gradients = step_III_gradient_calculation(\n",
    "            delta_output, delta_hidden, a_hidden, network_input,\n",
    "            weight_gradients, bias_gradients\n",
    "        )\n",
    "        \n",
    "        # Step IV. Update model parameters using gradients\n",
    "        weights_orig = copy.copy(weights)\n",
    "        biases_orig  = copy.copy(biases)\n",
    "        weights, biases = step_IV_update_parameters(\n",
    "            weights, biases, weight_gradients, bias_gradients, learning_rate\n",
    "        )\n",
    "        if i==iteration: # random\n",
    "            print(f\"i: {i}\")\n",
    "            display_htmlFcn2(\"weights_orig['w_1']\", \"weight_gradients['w_1']\", \"[learning_rate]\",\n",
    "                             \"weights_orig['w_1'] - weight_gradients['w_1'] * learning_rate\", \"weights['w_1']\",  \n",
    "                             \"biases_orig['b_1']\", \"bias_gradients['b_1']\", \"[learning_rate]\",\n",
    "                             \"biases_orig['b_1'] - bias_gradients['b_1'] * learning_rate\", \"biases['b_1']\",\n",
    "                             _localVars=locals(), _applyOrApplyMap=softZeros, _float_prec=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe10dfc",
   "metadata": {},
   "source": [
    "#### Whole function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda1a33",
   "metadata": {},
   "source": [
    "Rest of `run_ann_training_simulation()` concerns:\n",
    " - plot: `get_prediction_surface()`, `get_prediction_series()`, \n",
    "```python \n",
    "weights_history[iteration] = copy.copy(weights)\n",
    "biases_history[iteration] = copy.copy(biases)\n",
    "loss_history.append(sum(obs_error))\n",
    "if problem_type in ('SIN', 'ABS'):\n",
    "    prediction_history[iteration] = get_prediction_series(prediction_x, weights, biases, g_activation)\n",
    "else:                              \n",
    "    prediction_history[iteration] = get_prediction_surface(prediction_surface_xy, weights, biases, g_activation)\n",
    "``` \n",
    " - learning rate update: \n",
    " \n",
    "```python\n",
    "for iteration in range(n_iterations):\n",
    "    obs_error = []\n",
    "    for network_input, target in zip(observations, targets):\n",
    "        ...\n",
    "    learning_rate *= .95\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_surface(pred_surface_xy, weights, biases, g_activation):\n",
    "    \"\"\"Calculates current prediction surface for classification problem. Used for visualization\"\"\"\n",
    "    prediction_surface = [step_I_forwardprop(xy, weights, biases, g_activation)[0] for xy in pred_surface_xy]\n",
    "    return np.array(prediction_surface).squeeze().reshape(PREDICTION_SURFACE_RESOLUTION, PREDICTION_SURFACE_RESOLUTION)\n",
    "\n",
    "def get_prediction_series(pred_x, weights, biases, g_activation):\n",
    "    \"\"\"Calculates current prediction series for regression problem. Used for visualization\"\"\"\n",
    "    return step_I_forwardprop(pred_x[:, None], weights, biases, g_activation)[0]\n",
    "\n",
    "def run_ann_training_simulation(\n",
    "    problem_type='AND',\n",
    "    n_hidden_units=0,\n",
    "    n_iterations=100,\n",
    "    n_observations=50,\n",
    "    learning_rate=3\n",
    "):\n",
    "    \"\"\"Simulate ANN training on one of the following problems:\n",
    "\n",
    "    Binary Classification:\n",
    "        \"AND\": noisy binary logical AND data distrubted as 2D datapoints\n",
    "        \"OR\": noisy binary logical OR data distrubted as 2D datapoints\n",
    "        \"XOR\": noisy binary logical XOR data distrubted as 2D datapoints\n",
    "        \"RING\": data are a mode of one binary class surronded by a ring of the other\n",
    "    Regression (2D)\n",
    "        \"SIN\": data are noisy observations around the sin function with a slight vertical offset\n",
    "        \"ABS\": data are noisy observations around teh absolute value function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem_type : str\n",
    "        One of the problem types listed above\n",
    "    n_hidden_units : int\n",
    "        The number of hidden units in the hidden layer. Zero indicates no hidden layer\n",
    "    n_iterations : int\n",
    "        The number of times to run through the training observations\n",
    "    n_observations : int\n",
    "        The number of data points or (or dataset replicas for classification) that are used\n",
    "        in the training dataset\n",
    "    learning_rage : float\n",
    "        The initial learning rate (annealing is applied at each iteration)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss_history : list[float]\n",
    "        The loss function at each iteration of training\n",
    "    prediction_history : dict\n",
    "        Network predictions over the range of the training input. Used for learning visualization.\n",
    "        Keys are are the iteration number. Values are either prediction surface for classification\n",
    "        problems, or prediction series for regression\n",
    "    weights_history : dict\n",
    "        For each iteration, a snapshot of the state of the parameters. Used for visualizing hidden\n",
    "        unit states at each iteration\n",
    "    biases_history : dict\n",
    "        For each iteration, a snapshot of the state of the biases. Used for visualization. Used for\n",
    "        visualizing hidden unit states at each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize problem data\n",
    "    if problem_type in ('SIN', 'ABS'):\n",
    "        observations, targets = generate_regression_data(problem_type, n_observations)\n",
    "    else:\n",
    "        obs_x, obs_y, targets = generate_classification_data(problem_type, n_observations)\n",
    "        observations = np.vstack([obs_x, obs_y]).T\n",
    "\n",
    "    # Initialize model parameters $\\theta$\n",
    "    n_output_dims = 1\n",
    "    n_obs, n_input_dims = observations.shape\n",
    "    weights, biases, weight_gradients, bias_gradients = initialize_network_parameters(\n",
    "        n_input_dims, n_hidden_units, n_output_dims\n",
    "    )\n",
    "\n",
    "    # Initialize problem-specific activation functions and their derivatives\n",
    "    g_activation = {}\n",
    "    g_activation_prime = {}\n",
    "    if problem_type in ('SIN', 'ABS'):  # regression using linear output (and optional tanh hidden) activations\n",
    "        g_activation['g_out'], g_activation_prime['g_out'], _ = activation_functions['linear']\n",
    "        if 'w_2' in weights:\n",
    "            g_activation['g_1'], g_activation_prime['g_1'], _ = activation_functions['tanh']\n",
    "    else:  # classification using all sigmoid activations\n",
    "        g_activation['g_out'], g_activation_prime['g_out'], _ = activation_functions['sigmoid']\n",
    "        if 'w_2' in weights:\n",
    "            g_activation['g_1'], g_activation_prime['g_1'], _ = activation_functions['sigmoid']\n",
    "\n",
    "    # Setup for learning history / visualization\n",
    "    loss_history = []\n",
    "    prediction_history = {}\n",
    "    weights_history = {}\n",
    "    biases_history = {}\n",
    "    if problem_type in ('SIN', 'ABS'):\n",
    "        prediction_x = np.linspace(-5, 5, PREDICTION_SURFACE_RESOLUTION)\n",
    "    else:\n",
    "        prediction_surface_range = np.linspace(-.5, 1.5, PREDICTION_SURFACE_RESOLUTION)\n",
    "        prediction_surface_x, prediction_surface_y = np.meshgrid(prediction_surface_range, prediction_surface_range)\n",
    "        prediction_surface_xy = [(x, y) for x, y in zip(prediction_surface_x.ravel(), prediction_surface_y.ravel())]\n",
    "\n",
    "    # Run the training\n",
    "    for iteration in range(n_iterations):\n",
    "        obs_error = []\n",
    "        for network_input, target in zip(observations, targets):\n",
    "            network_input = np.atleast_2d(network_input)\n",
    "\n",
    "            # Step I: Forward propagate input signal through the network,\n",
    "            # collecting activations and hidden states\n",
    "            a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(\n",
    "                network_input, weights, biases, g_activation\n",
    "            )\n",
    "\n",
    "            # Step II: Backpropagate error signal\n",
    "            delta_output, delta_hidden = step_II_backprop(\n",
    "                target, a_output, z_output, z_hidden, weights, g_activation_prime\n",
    "            )\n",
    "\n",
    "            # Step III. Calculate Error gradient w.r.t. parameters\n",
    "            weight_gradients, bias_gradients = step_III_gradient_calculation(\n",
    "                delta_output, delta_hidden, a_hidden, network_input,\n",
    "                weight_gradients, bias_gradients\n",
    "            )\n",
    "\n",
    "            # Step IV. Update model parameters using gradients\n",
    "            weights, biases = step_IV_update_parameters(\n",
    "                weights, biases, weight_gradients, bias_gradients, learning_rate\n",
    "            )\n",
    "\n",
    "            # Keep track of observation error for loss history\n",
    "            obs_error.append(error_function(a_output, target))\n",
    "\n",
    "        # Anneal the learning rate (helps learning)\n",
    "        learning_rate *= .95\n",
    "\n",
    "        # Keep learning history for visualization\n",
    "        weights_history[iteration] = copy.copy(weights)\n",
    "        biases_history[iteration] = copy.copy(biases)\n",
    "        loss_history.append(sum(obs_error))\n",
    "        if problem_type in ('SIN', 'ABS'):\n",
    "            prediction_history[iteration] = get_prediction_series(prediction_x, weights, biases, g_activation)\n",
    "        else:\n",
    "            prediction_history[iteration] = get_prediction_surface(prediction_surface_xy, weights, biases, \n",
    "                                                                   g_activation)\n",
    "    return loss_history, prediction_history, weights_history, biases_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4830547",
   "metadata": {},
   "source": [
    "Auxiliar functions and contstants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69429671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function(prediction, target=1):\n",
    "    \"\"\"Squared error function (f(x) - y)**2\"\"\"\n",
    "    return (prediction - target)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5a8ec",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c0cfe",
   "metadata": {},
   "source": [
    "Let's see the output of `run_ann_training_simulation` in an ANN without hidden layers that performs an $OR$ classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc577cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN_UNITS = 0\n",
    "PROBLEM_TYPE = 'OR'\n",
    "n_iterations=100\n",
    "loss_history, prediction_history, weights_history, biases_history = run_ann_training_simulation(\n",
    "    problem_type=PROBLEM_TYPE,\n",
    "    n_hidden_units=N_HIDDEN_UNITS,\n",
    "    n_iterations=n_iterations,\n",
    "    learning_rate=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ed638",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_1D = np.array(loss_history).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b939eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [weights_history.get(i).get('w_1', []) for i in range(n_iterations)]\n",
    "w1 = [k[0][0] for k in w]\n",
    "w2 = [k[1][0] for k in w]\n",
    "b = [biases_history.get(i).get('b_1')[0][0] for i in range(n_iterations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, sharex=True, figsize=(12, 4))\n",
    "\n",
    "y = copy.deepcopy(loss_history_1D)\n",
    "n_obs = 200 # {run_ann_training_simulation() has default n_observations=50} & {OR} --> len(obs_x) = 4*50 = 200\n",
    "y /= n_obs # mean error in each input. Revert loss_history.append(sum(obs_error)) of run_ann_training_simulation()\n",
    "\n",
    "i_split = 25\n",
    "nan1 = np.full(n_iterations - i_split, np.NaN)\n",
    "nan2 = np.full(i_split, np.NaN)\n",
    "y1 = np.concatenate((y[:i_split], nan1))\n",
    "y2 = np.concatenate((nan2, y[i_split:]))\n",
    "\n",
    "# different scales\n",
    "axs[0,0].semilogy(y1)\n",
    "axs[0,0].semilogy(y2, '--', color='g')\n",
    "axs[1,0].semilogy(y2, color='g')\n",
    "axs[0,0].set_title('Mean Error')\n",
    "axs[1,0].set_ylabel(f'Zoomed from iter {i_split} on')\n",
    "axs[1,0].set_xlabel('Iter')\n",
    "\n",
    "axs[0,1].semilogy(w1, label='$w_{i_{1}j_{1}}$')\n",
    "axs[0,1].semilogy(w2, label='$w_{i_{2}j_{1}}$')\n",
    "axs[1,1].plot(b, label='$b_{j_{1}}$')\n",
    "axs[0,1].legend()\n",
    "axs[1,1].legend()\n",
    "\n",
    "plt.tight_layout(h_pad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_htmlFcn2(\"loss_history_1D[:5]\", \"prediction_history[0][:5]\", \"[list(prediction_history.keys())[i] for i in [0,1,2,-2,-1]]\",\n",
    "                 _localVars=locals(), _float_prec=[1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed345aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = list(weights_history.keys())[-1] # key_last_iter\n",
    "w = weights_history.get(i).get('w_1', [])\n",
    "b = biases_history.get(i).get('b_1', [])\n",
    "w = np.array(w)[np.newaxis,:]\n",
    "plotNN01([2,1], weights=w, biases=b, figsize=(4,3), \n",
    "         arrow_color_fcn=lambda x: \"green\" if x > 0 else \"red\",arrow_width_variable = True,arrow_width = 0.001*50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights_history.get(i)\n",
    "b = biases_history.get(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observations_and_g(problem_type=PROBLEM_TYPE, n_obs_per_class=5):\n",
    "    # Initialize problem data\n",
    "    obs_x, obs_y, targets = generate_classification_data(problem_type, n_obs_per_class)\n",
    "    observations = np.vstack([obs_x, obs_y]).T\n",
    "\n",
    "    # Initialize problem-specific activation functions and their derivatives\n",
    "    g_activation = {}\n",
    "    g_activation_prime = {}\n",
    "    g_activation['g_out'], g_activation_prime['g_out'], _ = activation_functions['sigmoid']\n",
    "    \n",
    "    return observations, targets, g_activation, g_activation_prime\n",
    "\n",
    "observations, targets, g_activation, g_activation_prime = get_observations_and_g()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a2686",
   "metadata": {},
   "source": [
    "Test weights and biases manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc758a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "_targets = [0,1,1,1]\n",
    "_observations = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "for o,t in zip(_observations,_targets):\n",
    "    z = o @ w['w_1'] + b['b_1']\n",
    "    prediction = g_activation['g_out'](z[0])[0]\n",
    "    print(f\"inputs {o} should output {t}\\tbut errors {(prediction - t)**2:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37f423",
   "metadata": {},
   "source": [
    "Test weights and biases programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cfdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_errors_and_output(observations=observations, targets=targets, weights=w, biases=b, g_activation=g_activation):\n",
    "\n",
    "    a_outputArr = []\n",
    "    errorArr = []\n",
    "    for i, (network_input, target) in enumerate(zip(observations, targets)):\n",
    "        network_input = np.atleast_2d(network_input)\n",
    "\n",
    "        # Step I: Forward propagate input signal through the network,\n",
    "        # collecting activations and hidden states\n",
    "        a_output, z_output, a_hidden, z_hidden = step_I_forwardprop(\n",
    "            network_input, weights, biases, g_activation\n",
    "        )\n",
    "\n",
    "        error = error_function(a_output, target) # (prediction - target)**2\n",
    "\n",
    "        a_outputArr.append([k[0] for k in a_output])\n",
    "        errorArr.append([k[0] for k in error])\n",
    "\n",
    "    return a_outputArr, np.array(errorArr).flatten()\n",
    "\n",
    "def plot_NN_errors(errorArr, ax, set_x=False):\n",
    "    n = len(errorArr)\n",
    "    bin_edges = np.logspace(np.log10(min(errorArr)), np.log10(max(errorArr)), num=int(n/4)) # log spaced bin edges\n",
    "    ax.hist(errorArr, bins=bin_edges, alpha=0.5, histtype='stepfilled', color='steelblue', edgecolor='k');\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'observations = {n}') \n",
    "    ax.tick_params(labelsize=7); ax.set_yticks(ax.get_yticks()[::2])\n",
    "    if set_x:\n",
    "        ax.set_xlabel('Error (log scale)')\n",
    "    return ax\n",
    "\n",
    "a_outputArr, errorArr = NN_errors_and_output()\n",
    "\n",
    "observations_big, targets_big, _, _ = get_observations_and_g(n_obs_per_class=200)\n",
    "a_output_big, errorArr_big = NN_errors_and_output(observations_big, targets_big)\n",
    "\n",
    "plt.ioff() # prevent plots from being displayed in the output of Jupyter Notebook\n",
    "fig, axs = plt.subplots(2,1, sharex=True) \n",
    "axs[0] = plot_NN_errors(errorArr, axs[0])\n",
    "axs[1] = plot_NN_errors(errorArr_big, axs[1], set_x=True)\n",
    "plt.tight_layout(h_pad=2)\n",
    "plt.ion() # revert plt.ioff()\n",
    "html_plot_fig = display_htmlFcn_plots(fig, width=500.0, height=500.0, caption=\"errors of each single observation\")\n",
    "\n",
    "print(f\"PROBLEM_TYPE: {PROBLEM_TYPE}\")\n",
    "display_htmlFcn2(\"observations\", \"np.rint(observations)\", \"targets\", \"a_outputArr\", \"errorArr\", html_plot_fig,\n",
    "                 _localVars=locals(), _applyOrApplyMap=[None,softZeros,softZeros,None,{\"apply\":highlight_maxFcn}],\n",
    "                 _formatVal=[\"{:.1f}\",\"{:.0f}\",\"{:.0f}\",\"{:.1e}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5aaa73",
   "metadata": {},
   "source": [
    "### `visualize_classification_learning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_main_classif02 = lambda *args, **kwargs: ann_main_classif(generate_classification_data,\n",
    "                                                            run_ann_training_simulation,\n",
    "                                                            plotNN02,\n",
    "                                                            *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca762d",
   "metadata": {},
   "source": [
    "#### `AND` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f2259",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ann_main_classif02('AND', N_HIDDEN_UNITS = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc3112",
   "metadata": {},
   "source": [
    "#### `XOR` example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d455c6",
   "metadata": {},
   "source": [
    "What about more complex categorization criterion that cannot be represented by a single plane? An example of a more complex binary classification criterion is the XOR operator. Here we can **NOT** achieve an overall **low error** if `N_HIDDEN_UNITS = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c97657",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ann_main_classif02('XOR', N_HIDDEN_UNITS = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e51bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ann_main_classif02('XOR', N_HIDDEN_UNITS = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089af19",
   "metadata": {},
   "source": [
    "Below we instead train a two-layer (i.e. single-hidden-layer) neural network on the XOR dataset. The network incorporates a hidden layer with 4 hidden units  (`N_HIDDEN_UNITS = 4`) and logistic sigmoid activation functions for all units in the hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a56237",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ann_main_classif02('XOR', N_HIDDEN_UNITS = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e63aa7",
   "metadata": {},
   "source": [
    "Final weights and biases are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history, prediction_history, weights_history, biases_history = run_ann_training_simulation(\n",
    "    problem_type='XOR',\n",
    "    n_hidden_units=4,\n",
    "    n_iterations=100,\n",
    "    learning_rate=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNN02(weights_history, biases_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b25017",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotNN02(weights_history, biases_history, \n",
    "        kwargs=dict(arrow_color_fcn=lambda x: \"green\" if x > 0 else \"red\",\n",
    "                    arrow_width_variable = True,arrow_width = 0.001*50,\n",
    "                   showFormulas=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fe5cb",
   "metadata": {},
   "source": [
    "#### `RING` example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277778c5",
   "metadata": {},
   "source": [
    "Figure 12 shows the results for learning a even more difficult nonlinear categorization function: points in and around $(x1,x2)=(0.5,0.5)$ are categorized as 1, while points in a ring surrounding the 1 datapoints are categorized as a 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d0f22",
   "metadata": {},
   "source": [
    "Note: in [website](https://dustinstansbury.github.io/theclevermachine/a-gentle-introduction-to-neural-networks) it uses `learning_rate=2`, but actually it converges with `learning_rate=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2d543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ann_main_classif02('RING', N_HIDDEN_UNITS = 4, n_iterations=100, learning_rate=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history, prediction_history, weights_history, biases_history = run_ann_training_simulation(\n",
    "    problem_type='RING',\n",
    "    n_hidden_units=4,\n",
    "    n_iterations=100,\n",
    "    learning_rate=2,\n",
    ")\n",
    "plotNN02(weights_history, biases_history, \n",
    "        kwargs=dict(arrow_color_fcn=lambda x: \"green\" if x > 0 else \"red\",\n",
    "                    arrow_width_variable = True,arrow_width = 0.001*50,\n",
    "                   showFormulas=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118432b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
